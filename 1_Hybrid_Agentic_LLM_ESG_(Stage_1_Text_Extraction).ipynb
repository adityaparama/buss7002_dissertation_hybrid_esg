{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Stage 1\n",
        "# ===============================\n",
        "!apt-get -qq update && apt-get -qq install -y poppler-utils tesseract-ocr >/dev/null\n",
        "!pip -q install pypdf pdf2image pytesseract nltk pandas pyarrow tqdm\n",
        "\n",
        "# ---------- Safe (re)mount ----------\n",
        "from google.colab import drive\n",
        "import os, time, subprocess, shutil\n",
        "\n",
        "MOUNTPOINT = \"/content/drive\"\n",
        "\n",
        "def _is_mounted(path):\n",
        "    try:\n",
        "        return os.path.exists(os.path.join(path, \"MyDrive\"))\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def ensure_drive_mounted():\n",
        "    if _is_mounted(MOUNTPOINT):\n",
        "        return\n",
        "    try:\n",
        "        subprocess.run([\"fusermount\", \"-u\", MOUNTPOINT], check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    except Exception:\n",
        "        pass\n",
        "    time.sleep(0.5)\n",
        "    shutil.rmtree(MOUNTPOINT, ignore_errors=True)\n",
        "    drive.mount(MOUNTPOINT, force_remount=False)\n",
        "\n",
        "ensure_drive_mounted()\n",
        "print(\"âœ… Drive ready.\")\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import re, hashlib\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pypdf import PdfReader\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import nltk; nltk.download('punkt', quiet=True)\n",
        "import nltk; nltk.download('punkt_tab', quiet=True)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "# If this path is wrong, the code will auto-find a folder named \"FORTUNE 500\" under your Drive.\n",
        "IN_DIR           = \"<<Google Drive Folder>>/FORTUNE 500\"\n",
        "SKIP_EXISTING    = True\n",
        "NATIVE_MIN_CHARS = 120\n",
        "OCR_DPI          = 300\n",
        "SENT_MINLEN      = 25\n",
        "\n",
        "# ---------- Regex & helpers ----------\n",
        "YEAR_RE     = re.compile(r'_(19|20)\\d{2}(?=\\.pdf$)', re.I)\n",
        "PAGINATION  = re.compile(r\"\\b(page|pp\\.|p\\.|appendix|annex|exhibit|figure|table|chapter|section|see page)\\b\", re.I)\n",
        "TOC_LINE    = re.compile(r\"\\.{3,}\\s*\\d{1,4}$\")\n",
        "BARE_NUM    = re.compile(r\"^\\s*\\d{1,3}\\s*$\")\n",
        "DATE_ONLY   = re.compile(r\"^\\s*(?:FY\\s*\\d{4}|(19|20)\\d{2}|Q[1-4])\\s*$\", re.I)\n",
        "SHORT_HEAD  = re.compile(r\"^(?:contents|table of contents|index)$\", re.I)\n",
        "\n",
        "def extract_year(p: Path):\n",
        "    m = YEAR_RE.search(p.name); return int(m.group(0)[1:]) if m else None\n",
        "\n",
        "def extract_type(p: Path):\n",
        "    n = p.name.upper()\n",
        "    if \"_AR_\" in n: return \"AR\"\n",
        "    if \"_10K_\" in n: return \"10K\"\n",
        "    return \"ESG\"\n",
        "\n",
        "def sha256_16(p: Path):\n",
        "    import hashlib\n",
        "    h=hashlib.sha256()\n",
        "    with open(p,'rb') as f:\n",
        "        for b in iter(lambda:f.read(1<<20), b''): h.update(b)\n",
        "    return h.hexdigest()[:16]\n",
        "\n",
        "def _keep_line(line: str) -> bool:\n",
        "    s = line.strip()\n",
        "    if not s: return False\n",
        "    if BARE_NUM.match(s): return False\n",
        "    if TOC_LINE.search(s): return False\n",
        "    if SHORT_HEAD.match(s): return False\n",
        "    if PAGINATION.search(s) and len(s.split()) <= 6: return False\n",
        "    if DATE_ONLY.match(s): return False\n",
        "    return True\n",
        "\n",
        "def clean_lines(text: str) -> str:\n",
        "    lines = text.splitlines()\n",
        "    lines = [ln for ln in lines if _keep_line(ln)]\n",
        "    return re.sub(r\"\\s+\", \" \", \"\\n\".join(lines)).strip()\n",
        "\n",
        "def to_sentences(txt: str):\n",
        "    txt = clean_lines(txt)\n",
        "    return [s.strip() for s in sent_tokenize(txt) if len(s.strip()) >= SENT_MINLEN]\n",
        "\n",
        "def native_text(pdf: Path) -> str:\n",
        "    try:\n",
        "        pages = PdfReader(str(pdf)).pages\n",
        "        parts = [(pg.extract_text() or \"\") for pg in pages]\n",
        "        raw = \"\\n\".join([t for t in parts if t]).strip()\n",
        "        return clean_lines(raw)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def ocr_fulldoc(pdf: Path) -> str:\n",
        "    try:\n",
        "        imgs = convert_from_path(str(pdf), dpi=OCR_DPI)\n",
        "        txts = [pytesseract.image_to_string(im, lang=\"eng\", config=\"--oem 1 --psm 6\").strip() for im in imgs]\n",
        "        raw = \"\\n\".join([t for t in txts if t]).strip()\n",
        "        return clean_lines(raw)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# ---------- Robust resume helpers ----------\n",
        "def read_manifest_safe(p: Path) -> pd.DataFrame:\n",
        "    if not p.exists():\n",
        "        return pd.DataFrame()\n",
        "    try:\n",
        "        return pd.read_parquet(p)\n",
        "    except Exception:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def can_skip_pdf(filename: str, text_path: Path, sent_path: Path, existing_manifest: pd.DataFrame) -> bool:\n",
        "    if text_path.exists() and sent_path.exists():\n",
        "        return True\n",
        "    if not existing_manifest.empty and filename in existing_manifest[\"filename\"].values:\n",
        "        row = existing_manifest.loc[existing_manifest[\"filename\"] == filename].iloc[-1]\n",
        "        txt_ok  = Path(row[\"text_path\"]).exists()\n",
        "        sent_ok = Path(row[\"sentences_path\"]).exists()\n",
        "        return txt_ok and sent_ok\n",
        "    return False\n",
        "\n",
        "def is_error_empty(filename: str, existing_manifest: pd.DataFrame) -> bool:\n",
        "    if existing_manifest.empty or filename not in existing_manifest[\"filename\"].values:\n",
        "        return False\n",
        "    status = existing_manifest.loc[existing_manifest[\"filename\"] == filename].iloc[-1][\"status\"]\n",
        "    return status == \"error_empty\"\n",
        "\n",
        "# ---------- Locate the root folder robustly ----------\n",
        "MYDRIVE = Path(\"/content/drive/MyDrive\")\n",
        "in_root = Path(IN_DIR)\n",
        "\n",
        "if not in_root.exists():\n",
        "    print(f\"âš ï¸ Provided IN_DIR not found:\\n  {in_root}\")\n",
        "    # Try a quick heuristic (Award â†’ Awards)\n",
        "    alt = Path(str(in_root).replace(\"Australia Award Scholarship\", \"Australia Awards Scholarship\"))\n",
        "    if alt.exists():\n",
        "        in_root = alt\n",
        "        print(f\"âž¡ï¸ Using alternate path:\\n  {in_root}\")\n",
        "    else:\n",
        "        # Search for a folder literally named \"FORTUNE 500\" under MyDrive\n",
        "        print(\"ðŸ”Ž Searching for a folder named 'FORTUNE 500' under MyDrive ...\")\n",
        "        candidates = [p for p in MYDRIVE.rglob(\"*\") if p.is_dir() and p.name == \"FORTUNE 500\"]\n",
        "        if not candidates:\n",
        "            # Also try case-insensitive match, just in case\n",
        "            candidates = [p for p in MYDRIVE.rglob(\"*\") if p.is_dir() and p.name.lower() == \"fortune 500\"]\n",
        "        if candidates:\n",
        "            in_root = candidates[0]\n",
        "            print(f\"âœ… Found:\\n  {in_root}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"Could not locate 'FORTUNE 500' folder. Please check the path or rename IN_DIR.\")\n",
        "\n",
        "# ---------- Discover PDFs case-insensitively & group by company folder ----------\n",
        "all_paths = list(in_root.rglob(\"*\"))\n",
        "all_pdfs = [p for p in all_paths if p.is_file() and p.suffix.lower() == \".pdf\"]\n",
        "\n",
        "# Sanity preview\n",
        "print(f\"\\nðŸ“‚ Using root: {in_root}\")\n",
        "print(f\"ðŸ”¢ PDFs discovered: {len(all_pdfs)}\")\n",
        "print(\"ðŸ‘€ First 10 PDFs:\")\n",
        "for p in all_pdfs[:10]:\n",
        "    print(\" -\", p)\n",
        "\n",
        "groups = defaultdict(list)\n",
        "for pdf in all_pdfs:\n",
        "    groups[pdf.parent].append(pdf)\n",
        "\n",
        "print(f\"\\nðŸ¢ Company folders with PDFs: {len(groups)}\")\n",
        "print(\"ðŸ‘€ First 10 folders:\")\n",
        "for i, folder in enumerate(list(groups.keys())[:10]):\n",
        "    print(\" -\", folder)\n",
        "print()\n",
        "\n",
        "# ---------- Process per folder ----------\n",
        "total_processed = 0\n",
        "total_skipped   = 0\n",
        "per_folder_summaries = []\n",
        "\n",
        "for folder, pdfs in groups.items():\n",
        "    out_root = folder / \"esg_stage1\"\n",
        "    texts_dir = out_root / \"texts\"\n",
        "    sents_dir = out_root / \"sentences\"\n",
        "    texts_dir.mkdir(parents=True, exist_ok=True)\n",
        "    sents_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    manifest_path = out_root / \"manifest.parquet\"\n",
        "    existing = read_manifest_safe(manifest_path)\n",
        "\n",
        "    rows = []\n",
        "    skipped_here = 0\n",
        "    processed_here = 0\n",
        "\n",
        "    for pdf in tqdm(pdfs, desc=f\"Processing PDFs in {folder.name}\", leave=False):\n",
        "        filename = pdf.name\n",
        "        hid = sha256_16(pdf)\n",
        "        text_path = texts_dir / f\"{hid}.txt\"\n",
        "        sent_path = sents_dir / f\"{hid}.parquet\"\n",
        "\n",
        "        if SKIP_EXISTING and can_skip_pdf(filename, text_path, sent_path, existing) and not is_error_empty(filename, existing):\n",
        "            skipped_here += 1\n",
        "            continue\n",
        "\n",
        "        year = extract_year(pdf); rtype = extract_type(pdf)\n",
        "\n",
        "        # 1) Native\n",
        "        txt = native_text(pdf)\n",
        "        if txt and len(txt) >= NATIVE_MIN_CHARS:\n",
        "            status = \"ok_native\"\n",
        "        else:\n",
        "            # 2) Full-doc OCR\n",
        "            txt = ocr_fulldoc(pdf)\n",
        "            status = \"ok_ocr\" if txt and len(txt) >= NATIVE_MIN_CHARS else \"error_empty\"\n",
        "\n",
        "        # Persist (even empty to keep manifest consistent)\n",
        "        text_path.write_text(txt or \"\", encoding=\"utf-8\")\n",
        "        pd.DataFrame({\"text\": to_sentences(txt) if txt else []}).to_parquet(sent_path, index=False)\n",
        "\n",
        "        rows.append({\n",
        "            \"sha256_16\": hid,\n",
        "            \"filename\": filename,\n",
        "            \"pdf_path\": str(pdf),\n",
        "            \"text_path\": str(text_path),\n",
        "            \"sentences_path\": str(sent_path),\n",
        "            \"status\": status,\n",
        "            \"doc_year\": year,\n",
        "            \"report_type\": rtype\n",
        "        })\n",
        "        processed_here += 1\n",
        "\n",
        "    # Merge+save manifest\n",
        "    if rows:\n",
        "        manifest = (pd.concat([existing, pd.DataFrame(rows)], ignore_index=True)\n",
        "                    if not existing.empty else pd.DataFrame(rows))\n",
        "        manifest = manifest.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
        "        manifest.to_parquet(manifest_path, index=False)\n",
        "    else:\n",
        "        manifest = existing if not existing.empty else pd.DataFrame(columns=[\n",
        "            \"sha256_16\",\"filename\",\"pdf_path\",\"text_path\",\"sentences_path\",\"status\",\"doc_year\",\"report_type\"\n",
        "        ])\n",
        "\n",
        "    per_folder_summaries.append({\n",
        "        \"folder\": str(folder),\n",
        "        \"pdfs_found\": len(pdfs),\n",
        "        \"processed_now\": processed_here,\n",
        "        \"skipped_existing\": skipped_here,\n",
        "        \"manifest_path\": str(manifest_path),\n",
        "        \"manifest_rows\": len(manifest)\n",
        "    })\n",
        "    total_processed += processed_here\n",
        "    total_skipped += skipped_here\n",
        "\n",
        "# ---------- Summary ----------\n",
        "print(\"\\nâœ… Done.\")\n",
        "print(f\"Folders: {len(groups)}  |  PDFs total: {len(all_pdfs)}\")\n",
        "print(f\"Processed now: {total_processed}  |  Skipped (existing): {total_skipped}\\n\")\n",
        "for s in per_folder_summaries[:15]:\n",
        "    print(f\"- {Path(s['folder']).name}: found={s['pdfs_found']}, processed={s['processed_now']}, \"\n",
        "          f\"skipped={s['skipped_existing']}, manifest_rows={s['manifest_rows']}\")\n",
        "    print(f\"  â†’ {s['manifest_path']}\")\n"
      ],
      "metadata": {
        "id": "yyHcgI8GQQSo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}