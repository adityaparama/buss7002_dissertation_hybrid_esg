{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Stage 1 — Text Extraction from PDF files\n",
        "# Read both native PDF and scanned PDF with OCR\n",
        "# ===============================\n",
        "!apt-get -qq update && apt-get -qq install -y poppler-utils tesseract-ocr >/dev/null\n",
        "!pip -q install pypdf pdf2image pytesseract nltk pandas pyarrow tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os, re, hashlib\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pypdf import PdfReader\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import nltk; nltk.download('punkt', quiet=True)\n",
        "import nltk; nltk.download('punkt_tab', quiet=True)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "IN_DIR           = \"<<Google Drive Link>>\"  # I uploaded the PDFs files in Google Drive. Each company has one folder.\n",
        "SKIP_EXISTING    = True                         # resumable\n",
        "NATIVE_MIN_CHARS = 120                          # if below → treat as scanned\n",
        "OCR_DPI          = 300\n",
        "SENT_MINLEN      = 25\n",
        "\n",
        "# ---------- Regex & helpers ----------\n",
        "YEAR_RE     = re.compile(r'_(19|20)\\d{2}(?=\\.pdf$)', re.I)\n",
        "PAGINATION  = re.compile(r\"\\b(page|pp\\.|p\\.|appendix|annex|exhibit|figure|table|chapter|section|see page)\\b\", re.I)\n",
        "TOC_LINE    = re.compile(r\"\\.{3,}\\s*\\d{1,4}$\")\n",
        "BARE_NUM    = re.compile(r\"^\\s*\\d{1,3}\\s*$\")\n",
        "DATE_ONLY   = re.compile(r\"^\\s*(?:FY\\s*\\d{4}|(19|20)\\d{2}|Q[1-4])\\s*$\", re.I)\n",
        "SHORT_HEAD  = re.compile(r\"^(?:contents|table of contents|index)$\", re.I)\n",
        "\n",
        "def extract_year(p: Path):\n",
        "    m = YEAR_RE.search(p.name); return int(m.group(0)[1:]) if m else None\n",
        "\n",
        "def extract_type(p: Path):\n",
        "    n = p.name.upper()\n",
        "    if \"_AR_\" in n: return \"AR\"\n",
        "    if \"_10K_\" in n: return \"10K\"\n",
        "    return \"ESG\"\n",
        "\n",
        "def sha256_16(p: Path):\n",
        "    h=hashlib.sha256()\n",
        "    with open(p,'rb') as f:\n",
        "        for b in iter(lambda:f.read(1<<20), b''): h.update(b)\n",
        "    return h.hexdigest()[:16]\n",
        "\n",
        "def _keep_line(line: str) -> bool:\n",
        "    s = line.strip()\n",
        "    if not s: return False\n",
        "    if BARE_NUM.match(s): return False\n",
        "    if TOC_LINE.search(s): return False\n",
        "    if SHORT_HEAD.match(s): return False\n",
        "    if PAGINATION.search(s) and len(s.split()) <= 6: return False\n",
        "    if DATE_ONLY.match(s): return False\n",
        "    return True\n",
        "\n",
        "def clean_lines(text: str) -> str:\n",
        "    lines = text.splitlines()\n",
        "    lines = [ln for ln in lines if _keep_line(ln)]\n",
        "    return re.sub(r\"\\s+\", \" \", \"\\n\".join(lines)).strip()\n",
        "\n",
        "def to_sentences(txt: str):\n",
        "    txt = clean_lines(txt)\n",
        "    return [s.strip() for s in sent_tokenize(txt) if len(s.strip()) >= SENT_MINLEN]\n",
        "\n",
        "def native_text(pdf: Path) -> str:\n",
        "    try:\n",
        "        pages = PdfReader(str(pdf)).pages\n",
        "        parts = [(pg.extract_text() or \"\") for pg in pages]\n",
        "        raw = \"\\n\".join([t for t in parts if t]).strip()\n",
        "        return clean_lines(raw)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def ocr_fulldoc(pdf: Path) -> str:\n",
        "    try:\n",
        "        imgs = convert_from_path(str(pdf), dpi=OCR_DPI)\n",
        "        txts = [pytesseract.image_to_string(im, lang=\"eng\", config=\"--oem 1 --psm 6\").strip() for im in imgs]\n",
        "        raw = \"\\n\".join([t for t in txts if t]).strip()\n",
        "        return clean_lines(raw)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# ---------- Discover PDFs & group by company folder ----------\n",
        "IN_ROOT = Path(IN_DIR)\n",
        "all_pdfs = sorted(IN_ROOT.rglob(\"*.pdf\"))\n",
        "groups = defaultdict(list)\n",
        "for pdf in all_pdfs:\n",
        "    groups[pdf.parent].append(pdf)\n",
        "\n",
        "total_processed = 0\n",
        "total_skipped = 0\n",
        "per_folder_summaries = []\n",
        "\n",
        "# ---------- Process per folder----------\n",
        "for folder, pdfs in groups.items():\n",
        "    out_root = folder / \"esg_stage1\"\n",
        "    texts_dir = out_root / \"texts\"\n",
        "    sents_dir = out_root / \"sentences\"\n",
        "    texts_dir.mkdir(parents=True, exist_ok=True)\n",
        "    sents_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    manifest_path = out_root / \"manifest.parquet\"\n",
        "    existing = pd.read_parquet(manifest_path) if (SKIP_EXISTING and manifest_path.exists()) else pd.DataFrame()\n",
        "\n",
        "    rows = []\n",
        "    skipped_here = 0\n",
        "    processed_here = 0\n",
        "\n",
        "    for pdf in tqdm(pdfs, desc=f\"Processing PDFs in {folder.name}\", leave=False):\n",
        "        filename = pdf.name\n",
        "        if SKIP_EXISTING and not existing.empty and filename in existing[\"filename\"].values:\n",
        "            skipped_here += 1\n",
        "            continue\n",
        "\n",
        "        year = extract_year(pdf); rtype = extract_type(pdf); hid = sha256_16(pdf)\n",
        "        text_path = texts_dir / f\"{hid}.txt\"\n",
        "        sent_path = sents_dir / f\"{hid}.parquet\"\n",
        "\n",
        "        # 1) Native extraction\n",
        "        txt = native_text(pdf)\n",
        "        if txt and len(txt) >= NATIVE_MIN_CHARS:\n",
        "            status = \"ok_native\"\n",
        "        else:\n",
        "            # 2) Full-document OCR\n",
        "            txt = ocr_fulldoc(pdf)\n",
        "            status = \"ok_ocr\" if txt and len(txt) >= NATIVE_MIN_CHARS else \"error_empty\"\n",
        "\n",
        "        # Save manifest extraction\n",
        "        text_path.write_text(txt or \"\", encoding=\"utf-8\")\n",
        "        pd.DataFrame({\"text\": to_sentences(txt) if txt else []}).to_parquet(sent_path, index=False)\n",
        "\n",
        "        rows.append({\n",
        "            \"sha256_16\": hid,\n",
        "            \"filename\": filename,\n",
        "            \"pdf_path\": str(pdf),\n",
        "            \"text_path\": str(text_path),\n",
        "            \"sentences_path\": str(sent_path),\n",
        "            \"status\": status,\n",
        "            \"doc_year\": year,\n",
        "            \"report_type\": rtype\n",
        "        })\n",
        "        processed_here += 1\n",
        "\n",
        "    # Merge+save manifest in the sampe folder\n",
        "    if rows:\n",
        "        manifest = (pd.concat([existing, pd.DataFrame(rows)], ignore_index=True)\n",
        "                    if not existing.empty else pd.DataFrame(rows))\n",
        "        manifest = manifest.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
        "        manifest.to_parquet(manifest_path, index=False)\n",
        "    else:\n",
        "        # If nothing new, keep existing manifest as-is\n",
        "        manifest = existing if not existing.empty else pd.DataFrame(columns=[\n",
        "            \"sha256_16\",\"filename\",\"pdf_path\",\"text_path\",\"sentences_path\",\"status\",\"doc_year\",\"report_type\"\n",
        "        ])\n",
        "\n",
        "    per_folder_summaries.append({\n",
        "        \"folder\": str(folder),\n",
        "        \"pdfs_found\": len(pdfs),\n",
        "        \"processed_now\": processed_here,\n",
        "        \"skipped_existing\": skipped_here,\n",
        "        \"manifest_path\": str(manifest_path),\n",
        "        \"manifest_rows\": len(manifest)\n",
        "    })\n",
        "    total_processed += processed_here\n",
        "    total_skipped += skipped_here\n",
        "\n",
        "# ---------- Result ----------\n",
        "print(\"\\n✅ Done.\")\n",
        "print(f\"Folders: {len(groups)}  |  PDFs total: {len(all_pdfs)}\")\n",
        "print(f\"Processed now: {total_processed}  |  Skipped (existing): {total_skipped}\\n\")\n",
        "for s in per_folder_summaries[:10]:\n",
        "    print(f\"- {Path(s['folder']).name}: found={s['pdfs_found']}, processed={s['processed_now']}, \"\n",
        "          f\"skipped={s['skipped_existing']}, manifest_rows={s['manifest_rows']}\")\n",
        "    print(f\"  → {s['manifest_path']}\")\n"
      ],
      "metadata": {
        "id": "xkvJrYX7_0dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "We6wfuvXtnxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AFTER REVISING THE TEXT EXTRACTION"
      ],
      "metadata": {
        "id": "J4RS2BLJtpS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Stage 1 — Text Extraction from PDF files (layout-aware, bullet-safe)\n",
        "# Read both native PDF and scanned PDF with OCR\n",
        "# Key changes:\n",
        "# - Preserve line breaks & bullets (no global whitespace collapse before tokenization)\n",
        "# - Strip common headers/footers/TOC lines per page\n",
        "# - Fix hyphenation at line ends\n",
        "# - Add defensive \"list-aware\" sentence splitting (bullets, long-lines fallback)\n",
        "# - Configurable Tesseract PSM for better list/column OCR\n",
        "# ===============================\n",
        "!apt-get -qq update && apt-get -qq install -y poppler-utils tesseract-ocr >/dev/null\n",
        "!pip -q install pypdf pdf2image pytesseract nltk pandas pyarrow tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os, re, hashlib, unicodedata\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pypdf import PdfReader\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import nltk; nltk.download('punkt', quiet=True)\n",
        "import nltk; nltk.download('punkt_tab', quiet=True)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "IN_DIR           = \"/content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/\"\n",
        "SKIP_EXISTING    = True                  # resumable\n",
        "NATIVE_MIN_CHARS = 120                   # if below → treat as scanned\n",
        "OCR_DPI          = 300\n",
        "OCR_PSM          = 6                     # 6: Assume a single uniform block of text; try 4 for columns if needed\n",
        "SENT_MINLEN      = 25\n",
        "MAX_SENT_LEN_CH  = 1200                  # if a \"sentence\" exceeds this, apply secondary splitting\n",
        "KEEP_PAGE_BREAKS = True                  # insert blank line between pages for native & OCR\n",
        "\n",
        "# ---------- Regex & helpers ----------\n",
        "YEAR_RE     = re.compile(r'_(19|20)\\d{2}(?=\\.pdf$)', re.I)\n",
        "PAGINATION  = re.compile(r\"\\b(page|pp\\.|p\\.|appendix|annex|exhibit|figure|table|chapter|section|see page)\\b\", re.I)\n",
        "TOC_LINE    = re.compile(r\"\\.{3,}\\s*\\d{1,4}$\")\n",
        "BARE_NUM    = re.compile(r\"^\\s*\\d{1,3}\\s*$\")\n",
        "DATE_ONLY   = re.compile(r\"^\\s*(?:FY\\s*\\d{4}|(19|20)\\d{2}|Q[1-4])\\s*$\", re.I)\n",
        "SHORT_HEAD  = re.compile(r\"^(?:contents|table of contents|index)$\", re.I)\n",
        "HEADER_FOOT_HINT = re.compile(r\"^(?:esg|sustainability|highlights|introduction|company|report|\\w+\\s+\\d{4})\\b\", re.I)\n",
        "\n",
        "BULLETS = (\"•\", \"▪\", \"‣\", \"‧\", \"・\", \"·\", \"-\", \"–\")  # last two treated cautiously\n",
        "\n",
        "def extract_year(p: Path):\n",
        "    m = YEAR_RE.search(p.name); return int(m.group(0)[1:]) if m else None\n",
        "\n",
        "def extract_type(p: Path):\n",
        "    n = p.name.upper()\n",
        "    if \"_AR_\" in n: return \"AR\"\n",
        "    if \"_10K_\" in n: return \"10K\"\n",
        "    return \"ESG\"\n",
        "\n",
        "def sha256_16(p: Path):\n",
        "    h=hashlib.sha256()\n",
        "    with open(p,'rb') as f:\n",
        "        for b in iter(lambda:f.read(1<<20), b''): h.update(b)\n",
        "    return h.hexdigest()[:16]\n",
        "\n",
        "def _likely_header_footer(line: str) -> bool:\n",
        "    s = line.strip()\n",
        "    if not s: return True\n",
        "    if BARE_NUM.match(s): return True\n",
        "    if TOC_LINE.search(s): return True\n",
        "    if SHORT_HEAD.match(s): return True\n",
        "    # very short lines with pagination cue\n",
        "    if PAGINATION.search(s) and len(s.split()) <= 6: return True\n",
        "    if DATE_ONLY.match(s): return True\n",
        "    # single ALL-CAPS short tokens (typical running heads)\n",
        "    if len(s) <= 40 and (s.isupper() or HEADER_FOOT_HINT.match(s)): return True\n",
        "    return False\n",
        "\n",
        "def _normalize_bullets(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Ensure bullets start new lines, so lists become separate segments.\n",
        "    Also fix cases like ' text • item ' -> '\\n• item'\n",
        "    \"\"\"\n",
        "    txt = text\n",
        "    # Make sure bullets at mid-line get their own line\n",
        "    for b in BULLETS:\n",
        "        # surround bullet with spaces to avoid gluing to words\n",
        "        txt = txt.replace(f\" {b} \", f\"\\n{b} \")\n",
        "        txt = txt.replace(f\"\\t{b} \", f\"\\n{b} \")\n",
        "        # bullet without trailing space\n",
        "        txt = txt.replace(f\" {b}\", f\"\\n{b}\")\n",
        "    # de-duplicate excessive newlines\n",
        "    txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt)\n",
        "    return txt\n",
        "\n",
        "def _fix_hyphenation(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Join words broken by line-end hyphenation: 'carbon-\\nneutral' -> 'carbon-neutral'\n",
        "    Only when hyphen is at end of line followed by a word start.\n",
        "    \"\"\"\n",
        "    return re.sub(r\"-\\n(?=\\w)\", \"\", text)\n",
        "\n",
        "def clean_page_text(page_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Per-page cleaning: remove obvious headers/footers/TOC lines,\n",
        "    preserve internal newlines, normalize bullets, fix hyphenation.\n",
        "    \"\"\"\n",
        "    if not page_text:\n",
        "        return \"\"\n",
        "    # Normalize unicode + keep as original case for better sentence detection later\n",
        "    t = unicodedata.normalize(\"NFKC\", page_text)\n",
        "    # Fix hyphenation first to avoid breaking words into odd tokens\n",
        "    t = _fix_hyphenation(t)\n",
        "\n",
        "    # Split into lines, drop typical headers/footers\n",
        "    lines = [ln for ln in t.splitlines() if ln.strip()]\n",
        "    if not lines:\n",
        "        return \"\"\n",
        "    # Drop first/last line if they look like running head/footer\n",
        "    if lines and _likely_header_footer(lines[0]): lines = lines[1:]\n",
        "    if lines and _likely_header_footer(lines[-1]): lines = lines[:-1]\n",
        "    # Filter remaining noisy lines\n",
        "    lines = [ln for ln in lines if not _likely_header_footer(ln)]\n",
        "    if not lines:\n",
        "        return \"\"\n",
        "\n",
        "    # Rejoin with preserved newlines\n",
        "    t = \"\\n\".join(lines)\n",
        "\n",
        "    # Normalize bullets to enforce line starts\n",
        "    t = _normalize_bullets(t)\n",
        "\n",
        "    # Trim stray spaces on each line, keep line structure\n",
        "    t = \"\\n\".join(ln.strip() for ln in t.splitlines())\n",
        "    return t.strip()\n",
        "\n",
        "def to_sentences_structured(txt: str):\n",
        "    \"\"\"\n",
        "    Structure-aware sentence segmentation:\n",
        "    1) Keep existing newlines as hard segment boundaries\n",
        "    2) If a segment is very long or contains bullets, secondary split by bullets/; / spaced dashes\n",
        "    3) Run NLTK sentence tokenizer inside each small segment\n",
        "    \"\"\"\n",
        "    if not txt:\n",
        "        return []\n",
        "    segments = [seg for seg in txt.splitlines() if seg.strip()]\n",
        "    out = []\n",
        "    for seg in segments:\n",
        "        # Secondary split if segment too long or seems list-like\n",
        "        needs_split = (len(seg) > MAX_SENT_LEN_CH) or any(b in seg for b in BULLETS)\n",
        "        if needs_split:\n",
        "            # split on bullets at start or mid sentence\n",
        "            split_bits = re.split(r\"(?<=\\S)\\s+(?=(?:[•▪‣‧・·]|- |– )\\s*)\", seg)\n",
        "        else:\n",
        "            split_bits = [seg]\n",
        "\n",
        "        for bit in split_bits:\n",
        "            b = bit.strip()\n",
        "            if not b:\n",
        "                continue\n",
        "            # Now apply sentence tokenizer\n",
        "            for s in sent_tokenize(b):\n",
        "                s2 = s.strip()\n",
        "                if len(s2) >= SENT_MINLEN:\n",
        "                    out.append(s2)\n",
        "    return out\n",
        "\n",
        "def native_text(pdf: Path) -> str:\n",
        "    \"\"\"\n",
        "    Extract per-page text with pypdf, then clean page-by-page.\n",
        "    We KEEP newlines and insert blank lines between pages if configured.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pages = PdfReader(str(pdf)).pages\n",
        "        cleaned_pages = []\n",
        "        for pg in pages:\n",
        "            raw = (pg.extract_text() or \"\")\n",
        "            cp = clean_page_text(raw)\n",
        "            if cp:\n",
        "                cleaned_pages.append(cp)\n",
        "        if not cleaned_pages:\n",
        "            return \"\"\n",
        "        glue = \"\\n\\n\" if KEEP_PAGE_BREAKS else \"\\n\"\n",
        "        return glue.join(cleaned_pages).strip()\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def ocr_fulldoc(pdf: Path) -> str:\n",
        "    \"\"\"\n",
        "    Full-document OCR with layout-friendly settings.\n",
        "    We keep per-image (page) text separate and clean page-by-page.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        imgs = convert_from_path(str(pdf), dpi=OCR_DPI)\n",
        "        cleaned_pages = []\n",
        "        for im in imgs:\n",
        "            txt = pytesseract.image_to_string(\n",
        "                im, lang=\"eng\",\n",
        "                config=f\"--oem 1 --psm {OCR_PSM}\"\n",
        "            ).strip()\n",
        "            cp = clean_page_text(txt)\n",
        "            if cp:\n",
        "                cleaned_pages.append(cp)\n",
        "        if not cleaned_pages:\n",
        "            return \"\"\n",
        "        glue = \"\\n\\n\" if KEEP_PAGE_BREAKS else \"\\n\"\n",
        "        return glue.join(cleaned_pages).strip()\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# ---------- Discover PDFs & group by company folder ----------\n",
        "IN_ROOT = Path(IN_DIR)\n",
        "all_pdfs = sorted(IN_ROOT.rglob(\"*.pdf\"))\n",
        "groups = defaultdict(list)\n",
        "for pdf in all_pdfs:\n",
        "    groups[pdf.parent].append(pdf)\n",
        "\n",
        "total_processed = 0\n",
        "total_skipped = 0\n",
        "per_folder_summaries = []\n",
        "\n",
        "# ---------- Process per folder ----------\n",
        "for folder, pdfs in groups.items():\n",
        "    out_root = folder / \"esg_stage1(ver2)\"\n",
        "    texts_dir = out_root / \"texts\"\n",
        "    sents_dir = out_root / \"sentences\"\n",
        "    texts_dir.mkdir(parents=True, exist_ok=True)\n",
        "    sents_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    manifest_path = out_root / \"manifest.parquet\"\n",
        "    existing = pd.read_parquet(manifest_path) if (SKIP_EXISTING and manifest_path.exists()) else pd.DataFrame()\n",
        "\n",
        "    rows = []\n",
        "    skipped_here = 0\n",
        "    processed_here = 0\n",
        "\n",
        "    for pdf in tqdm(pdfs, desc=f\"Processing PDFs in {folder.name}\", leave=False):\n",
        "        filename = pdf.name\n",
        "        if SKIP_EXISTING and not existing.empty and filename in existing[\"filename\"].values:\n",
        "            skipped_here += 1\n",
        "            continue\n",
        "\n",
        "        year = extract_year(pdf); rtype = extract_type(pdf); hid = sha256_16(pdf)\n",
        "        text_path = texts_dir / f\"{hid}.txt\"\n",
        "        sent_path = sents_dir / f\"{hid}.parquet\"\n",
        "\n",
        "        # 1) Native extraction (layout-preserving)\n",
        "        txt = native_text(pdf)\n",
        "        if txt and len(txt) >= NATIVE_MIN_CHARS:\n",
        "            status = \"ok_native\"\n",
        "        else:\n",
        "            # 2) Full-document OCR (page-by-page cleaning; layout-friendly PSM)\n",
        "            txt = ocr_fulldoc(pdf)\n",
        "            status = \"ok_ocr\" if txt and len(txt) >= NATIVE_MIN_CHARS else \"error_empty\"\n",
        "\n",
        "        # Save cleaned text and robust sentences\n",
        "        text_path.write_text(txt or \"\", encoding=\"utf-8\")\n",
        "        sentences = to_sentences_structured(txt) if txt else []\n",
        "        pd.DataFrame({\"text\": sentences}).to_parquet(sent_path, index=False)\n",
        "\n",
        "        rows.append({\n",
        "            \"sha256_16\": hid,\n",
        "            \"filename\": filename,\n",
        "            \"pdf_path\": str(pdf),\n",
        "            \"text_path\": str(text_path),\n",
        "            \"sentences_path\": str(sent_path),\n",
        "            \"status\": status,\n",
        "            \"doc_year\": year,\n",
        "            \"report_type\": rtype\n",
        "        })\n",
        "        processed_here += 1\n",
        "\n",
        "    # Merge+save manifest in the same folder\n",
        "    if rows:\n",
        "        manifest = (pd.concat([existing, pd.DataFrame(rows)], ignore_index=True)\n",
        "                    if not existing.empty else pd.DataFrame(rows))\n",
        "        manifest = manifest.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
        "        manifest.to_parquet(manifest_path, index=False)\n",
        "    else:\n",
        "        manifest = existing if not existing.empty else pd.DataFrame(columns=[\n",
        "            \"sha256_16\",\"filename\",\"pdf_path\",\"text_path\",\"sentences_path\",\"status\",\"doc_year\",\"report_type\"\n",
        "        ])\n",
        "\n",
        "    per_folder_summaries.append({\n",
        "        \"folder\": str(folder),\n",
        "        \"pdfs_found\": len(pdfs),\n",
        "        \"processed_now\": processed_here,\n",
        "        \"skipped_existing\": skipped_here,\n",
        "        \"manifest_path\": str(manifest_path),\n",
        "        \"manifest_rows\": len(manifest)\n",
        "    })\n",
        "    total_processed += processed_here\n",
        "    total_skipped += skipped_here\n",
        "\n",
        "# ---------- Result ----------\n",
        "print(\"\\n✅ Done.\")\n",
        "print(f\"Folders: {len(groups)}  |  PDFs total: {len(all_pdfs)}\")\n",
        "print(f\"Processed now: {total_processed}  |  Skipped (existing): {total_skipped}\\n\")\n",
        "for s in per_folder_summaries[:10]:\n",
        "    print(f\"- {Path(s['folder']).name}: found={s['pdfs_found']}, processed={s['processed_now']}, \"\n",
        "          f\"skipped={s['skipped_existing']}, manifest_rows={s['manifest_rows']}\")\n",
        "    print(f\"  → {s['manifest_path']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr0YWsyitotr",
        "outputId": "cbbb4de8-fc44-4223-e63b-c1278bdf374c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing PDFs in 012. Costco:  80%|████████  | 4/5 [01:31<00:15, 15.67s/it]WARNING:pypdf._reader:Ignoring wrong pointing object 1561 0 (offset 0)\n",
            "Processing PDFs in 014. Microsoft:  50%|█████     | 3/6 [00:56<01:06, 22.17s/it]WARNING:pypdf._reader:Ignoring wrong pointing object 593 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 3256 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 3383 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 3462 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 3474 0 (offset 0)\n",
            "Processing PDFs in 014. Microsoft:  83%|████████▎ | 5/6 [01:11<00:13, 13.55s/it]WARNING:pypdf._reader:Ignoring wrong pointing object 1215 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8158 0 (offset 0)\n",
            "                                                                             \n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STAGE 1 WITH IMPROVEMENT"
      ],
      "metadata": {
        "id": "GmWg-VD5DIcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Stage 1 — Text Extraction from PDF files (improved)\n",
        "# - Native PDF + OCR\n",
        "# - Soft line-break joining, hyphenation repair, header/TOC filtering\n",
        "# - Safer sentence tokenization (paragraph-aware)\n",
        "# ===============================\n",
        "!apt-get -qq update && apt-get -qq install -y poppler-utils tesseract-ocr >/dev/null\n",
        "!pip -q install pypdf pdf2image pytesseract nltk pandas pyarrow tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os, re, hashlib, unicodedata, string\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from pypdf import PdfReader\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "\n",
        "import nltk; nltk.download('punkt', quiet=True)\n",
        "import nltk; nltk.download('punkt_tab', quiet=True)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "IN_DIR           = \"/content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/\"\n",
        "SKIP_EXISTING    = True\n",
        "NATIVE_MIN_CHARS = 120\n",
        "OCR_DPI          = 300\n",
        "SENT_MINLEN      = 25\n",
        "\n",
        "# Soft line-break joiner knobs\n",
        "JOIN_SOFT_BREAKS     = True    # turn on the joiner\n",
        "KEEP_DOUBLE_NEWLINES = True    # preserve paragraph gaps\n",
        "MAX_JOIN_LINE_LEN    = 2000    # guardrail against runaway joins\n",
        "\n",
        "# Tesseract config (PSM 4 handles multi-column pages better than default 6 in many ESG PDFs)\n",
        "OCR_CONFIG = \"--oem 1 --psm 4 -c preserve_interword_spaces=1\"\n",
        "\n",
        "# ---------- Regex & helpers ----------\n",
        "YEAR_RE     = re.compile(r'_(19|20)\\d{2}(?=\\.pdf$)', re.I)\n",
        "PAGINATION  = re.compile(r\"\\b(page|pp\\.|p\\.|appendix|annex|exhibit|figure|table|chapter|section|see page)\\b\", re.I)\n",
        "TOC_LINE    = re.compile(r\"\\.{3,}\\s*\\d{1,4}$\")\n",
        "BARE_NUM    = re.compile(r\"^\\s*\\d{1,3}\\s*$\")\n",
        "DATE_ONLY   = re.compile(r\"^\\s*(?:FY\\s*\\d{4}|(19|20)\\d{2}|Q[1-4])\\s*$\", re.I)\n",
        "SHORT_HEAD  = re.compile(r\"^(?:contents|table of contents|index)$\", re.I)\n",
        "\n",
        "# sentence-final punctuation (also covers quotes and ellipsis)\n",
        "SENT_FINAL  = re.compile(r'[.?!…:;)\"’%]\\s*$')\n",
        "# bullet/heading detectors\n",
        "BULLET_START = re.compile(r\"^\\s*(?:[-–•‣●◦▪]|[0-9]{1,3}[.)]|[A-Z]{1,2}[.)])\\s+\")\n",
        "ALLCAPS_LINE = re.compile(r\"^[A-Z0-9][A-Z0-9 &/–\\-.,]{2,}$\")\n",
        "\n",
        "_SOFT_HYPHEN = \"\\u00AD\"\n",
        "PUNCT_TABLE  = str.maketrans(\"\", \"\", string.punctuation)\n",
        "\n",
        "def extract_year(p: Path):\n",
        "    m = YEAR_RE.search(p.name); return int(m.group(0)[1:]) if m else None\n",
        "\n",
        "def extract_type(p: Path):\n",
        "    n = p.name.upper()\n",
        "    if \"_AR_\" in n: return \"AR\"\n",
        "    if \"_10K_\" in n: return \"10K\"\n",
        "    return \"ESG\"\n",
        "\n",
        "def sha256_16(p: Path):\n",
        "    h=hashlib.sha256()\n",
        "    with open(p,'rb') as f:\n",
        "        for b in iter(lambda:f.read(1<<20), b''): h.update(b)\n",
        "    return h.hexdigest()[:16]\n",
        "\n",
        "def _keep_line(line: str) -> bool:\n",
        "    s = line.strip()\n",
        "    if not s: return False\n",
        "    if BARE_NUM.match(s): return False\n",
        "    if TOC_LINE.search(s): return False\n",
        "    if SHORT_HEAD.match(s): return False\n",
        "    # very short pagination-only lines\n",
        "    if PAGINATION.search(s) and len(s.split()) <= 6: return False\n",
        "    if DATE_ONLY.match(s): return False\n",
        "    return True\n",
        "\n",
        "def normalize_soft_hyphens(text: str) -> str:\n",
        "    # Remove discretionary hyphen glyphs and normalize spaces\n",
        "    text = text.replace(_SOFT_HYPHEN, \"\")\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    return text\n",
        "\n",
        "def is_bullet_or_heading(line: str) -> bool:\n",
        "    s = line.strip()\n",
        "    if not s: return False\n",
        "    if BULLET_START.match(s): return True\n",
        "    # short all-caps headings (avoid shouting paragraphs)\n",
        "    if len(s) <= 80 and ALLCAPS_LINE.match(s):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def should_join(prev: str, nxt: str) -> bool:\n",
        "    \"\"\"\n",
        "    Decide if prev and next lines are the same sentence (i.e., visual wrap).\n",
        "    Heuristics:\n",
        "      - prev NOT ending with sentence-final punctuation\n",
        "      - next starts lowercase or punctuation indicating continuation\n",
        "      - neither line looks like a bullet/heading\n",
        "    \"\"\"\n",
        "    if not prev or not nxt: return False\n",
        "    if is_bullet_or_heading(prev) or is_bullet_or_heading(nxt): return False\n",
        "\n",
        "    prev_stripped = prev.rstrip()\n",
        "    nxt_stripped  = nxt.lstrip()\n",
        "\n",
        "    # hyphenated word wrap always joins (handled separately, but we allow here too)\n",
        "    if prev_stripped.endswith(\"-\") and (nxt_stripped[:1].isalpha()):\n",
        "        return True\n",
        "\n",
        "    # already ends a sentence? keep the break\n",
        "    if SENT_FINAL.search(prev_stripped):\n",
        "        return False\n",
        "\n",
        "    # continuation cues: lowercase start, comma/semicolon, closing bracket, quote, or small function words\n",
        "    if re.match(r'^[a-z“’)\\],;:—–-]', nxt_stripped):\n",
        "        return True\n",
        "\n",
        "    # sentence likely continues if prev ends with a word char and next starts with a lowercase or “of/and/to/for/in”\n",
        "    if re.search(r'[A-Za-z0-9]$', prev_stripped) and re.match(r'^(?:[a-z]|of|and|to|for|in|on|with|as)\\b', nxt_stripped):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def join_soft_linebreaks(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert single newlines inside paragraphs to spaces, keep double newlines as paragraph breaks.\n",
        "    Also repairs hyphenation at line boundaries.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # keep paragraph blocks separated by blank lines\n",
        "    blocks = re.split(r'\\n{2,}', text)\n",
        "    fixed_blocks = []\n",
        "\n",
        "    for block in blocks:\n",
        "        lines = [ln for ln in block.splitlines() if _keep_line(ln)]\n",
        "        if not lines:\n",
        "            fixed_blocks.append(\"\")\n",
        "            continue\n",
        "\n",
        "        rebuilt = []\n",
        "        cur = lines[0].rstrip()\n",
        "\n",
        "        for i in range(1, len(lines)):\n",
        "            nxt = lines[i].strip()\n",
        "\n",
        "            # hyphenated join: \"hard-to-decarbonize-\\nregions\" -> \"hard-to-decarbonize regions\"\n",
        "            if cur.endswith(\"-\") and (nxt[:1].isalpha()):\n",
        "                cur = cur[:-1] + nxt\n",
        "                continue\n",
        "\n",
        "            if JOIN_SOFT_BREAKS and should_join(cur, nxt) and len(cur) < MAX_JOIN_LINE_LEN:\n",
        "                cur = cur + \" \" + nxt\n",
        "            else:\n",
        "                rebuilt.append(cur)\n",
        "                cur = nxt\n",
        "\n",
        "        rebuilt.append(cur)\n",
        "\n",
        "        fixed = \"\\n\".join(rebuilt)\n",
        "        fixed_blocks.append(fixed)\n",
        "\n",
        "    if KEEP_DOUBLE_NEWLINES:\n",
        "        return \"\\n\\n\".join(fixed_blocks).strip()\n",
        "    else:\n",
        "        return \"\\n\".join(fixed_blocks).strip()\n",
        "\n",
        "def clean_lines_keep_structure(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Apply line filtering, soft-break joining, and whitespace normalization while\n",
        "    preserving paragraph gaps.\n",
        "    \"\"\"\n",
        "    text = normalize_soft_hyphens(text)\n",
        "    text = join_soft_linebreaks(text)\n",
        "    # collapse 3+ blank lines to 2\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    # trim trailing spaces on lines\n",
        "    text = \"\\n\".join([ln.rstrip() for ln in text.splitlines()])\n",
        "    return text.strip()\n",
        "\n",
        "def paragraphs_to_sentences(txt: str):\n",
        "    \"\"\"\n",
        "    Tokenize by NLTK after paragraph-aware joining.\n",
        "    \"\"\"\n",
        "    if not txt:\n",
        "        return []\n",
        "    # keep paragraphs for better sentence context\n",
        "    paras = [p.strip() for p in re.split(r'\\n{2,}', txt) if p.strip()]\n",
        "    sents = []\n",
        "    for p in paras:\n",
        "        # replace remaining single newlines in a paragraph with spaces (safety)\n",
        "        p_flat = re.sub(r'\\n+', ' ', p).strip()\n",
        "        sents.extend([s.strip() for s in sent_tokenize(p_flat) if len(s.strip()) >= SENT_MINLEN])\n",
        "    return sents\n",
        "\n",
        "def native_text(pdf: Path) -> str:\n",
        "    try:\n",
        "        pages = PdfReader(str(pdf)).pages\n",
        "        parts = [(pg.extract_text() or \"\") for pg in pages]\n",
        "        raw = \"\\n\\n\".join([t for t in parts if t]).strip()  # keep page gaps as paragraph gaps\n",
        "        return clean_lines_keep_structure(raw)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def ocr_fulldoc(pdf: Path) -> str:\n",
        "    try:\n",
        "        imgs = convert_from_path(str(pdf), dpi=OCR_DPI)\n",
        "        txts = [pytesseract.image_to_string(im, lang=\"eng\", config=OCR_CONFIG).strip() for im in imgs]\n",
        "        raw = \"\\n\\n\".join([t for t in txts if t]).strip()\n",
        "        return clean_lines_keep_structure(raw)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def to_sentences(txt: str):\n",
        "    txt = clean_lines_keep_structure(txt)\n",
        "    return paragraphs_to_sentences(txt)\n",
        "\n",
        "# ---------- Discover PDFs & group by company folder ----------\n",
        "IN_ROOT = Path(IN_DIR)\n",
        "all_pdfs = sorted(IN_ROOT.rglob(\"*.pdf\"))\n",
        "groups = defaultdict(list)\n",
        "for pdf in all_pdfs:\n",
        "    groups[pdf.parent].append(pdf)\n",
        "\n",
        "total_processed = 0\n",
        "total_skipped = 0\n",
        "per_folder_summaries = []\n",
        "\n",
        "# ---------- Process per folder ----------\n",
        "for folder, pdfs in groups.items():\n",
        "    out_root = folder / \"esg_stage1(ver3)\"\n",
        "    texts_dir = out_root / \"texts\"\n",
        "    sents_dir = out_root / \"sentences\"\n",
        "    texts_dir.mkdir(parents=True, exist_ok=True)\n",
        "    sents_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    manifest_path = out_root / \"manifest.parquet\"\n",
        "    existing = pd.read_parquet(manifest_path) if (SKIP_EXISTING and manifest_path.exists()) else pd.DataFrame()\n",
        "\n",
        "    rows = []\n",
        "    skipped_here = 0\n",
        "    processed_here = 0\n",
        "\n",
        "    for pdf in tqdm(pdfs, desc=f\"Processing PDFs in {folder.name}\", leave=False):\n",
        "        filename = pdf.name\n",
        "        if SKIP_EXISTING and not existing.empty and filename in existing[\"filename\"].values:\n",
        "            skipped_here += 1\n",
        "            continue\n",
        "\n",
        "        year = extract_year(pdf); rtype = extract_type(pdf); hid = sha256_16(pdf)\n",
        "        text_path = texts_dir / f\"{hid}.txt\"\n",
        "        sent_path = sents_dir / f\"{hid}.parquet\"\n",
        "\n",
        "        # 1) Native extraction\n",
        "        txt = native_text(pdf)\n",
        "        if txt and len(txt) >= NATIVE_MIN_CHARS:\n",
        "            status = \"ok_native\"\n",
        "        else:\n",
        "            # 2) Full-document OCR\n",
        "            txt = ocr_fulldoc(pdf)\n",
        "            status = \"ok_ocr\" if txt and len(txt) >= NATIVE_MIN_CHARS else \"error_empty\"\n",
        "\n",
        "        # Save manifest extraction\n",
        "        text_path.write_text(txt or \"\", encoding=\"utf-8\")\n",
        "        pd.DataFrame({\"text\": to_sentences(txt) if txt else []}).to_parquet(sent_path, index=False)\n",
        "\n",
        "        rows.append({\n",
        "            \"sha256_16\": hid,\n",
        "            \"filename\": filename,\n",
        "            \"pdf_path\": str(pdf),\n",
        "            \"text_path\": str(text_path),\n",
        "            \"sentences_path\": str(sent_path),\n",
        "            \"status\": status,\n",
        "            \"doc_year\": year,\n",
        "            \"report_type\": rtype\n",
        "        })\n",
        "        processed_here += 1\n",
        "\n",
        "    # Merge+save manifest in the same folder\n",
        "    if rows:\n",
        "        manifest = (pd.concat([existing, pd.DataFrame(rows)], ignore_index=True)\n",
        "                    if not existing.empty else pd.DataFrame(rows))\n",
        "        manifest = manifest.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
        "        manifest.to_parquet(manifest_path, index=False)\n",
        "    else:\n",
        "        manifest = existing if not existing.empty else pd.DataFrame(columns=[\n",
        "            \"sha256_16\",\"filename\",\"pdf_path\",\"text_path\",\"sentences_path\",\"status\",\"doc_year\",\"report_type\"\n",
        "        ])\n",
        "\n",
        "    per_folder_summaries.append({\n",
        "        \"folder\": str(folder),\n",
        "        \"pdfs_found\": len(pdfs),\n",
        "        \"processed_now\": processed_here,\n",
        "        \"skipped_existing\": skipped_here,\n",
        "        \"manifest_path\": str(manifest_path),\n",
        "        \"manifest_rows\": len(manifest)\n",
        "    })\n",
        "    total_processed += processed_here\n",
        "    total_skipped += skipped_here\n",
        "\n",
        "# ---------- Result ----------\n",
        "print(\"\\n✅ Done.\")\n",
        "print(f\"Folders: {len(groups)}  |  PDFs total: {len(all_pdfs)}\")\n",
        "print(f\"Processed now: {total_processed}  |  Skipped (existing): {total_skipped}\\n\")\n",
        "for s in per_folder_summaries[:10]:\n",
        "    print(f\"- {Path(s['folder']).name}: found={s['pdfs_found']}, processed={s['processed_now']}, \"\n",
        "          f\"skipped={s['skipped_existing']}, manifest_rows={s['manifest_rows']}\")\n",
        "    print(f\"  → {s['manifest_path']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vcf-PcbDHy2",
        "outputId": "235d3290-f14d-45e9-ff9d-cec5b57f7372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                               "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Done.\n",
            "Folders: 493  |  PDFs total: 2278\n",
            "Processed now: 0  |  Skipped (existing): 2278\n",
            "\n",
            "- 000. Mini Trial: found=5, processed=0, skipped=5, manifest_rows=5\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/000. Mini Trial/esg_stage1(ver3)/manifest.parquet\n",
            "- 001. Walmart: found=6, processed=0, skipped=6, manifest_rows=6\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/001. Walmart/esg_stage1(ver3)/manifest.parquet\n",
            "- 002. Amazon: found=5, processed=0, skipped=5, manifest_rows=5\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/002. Amazon/esg_stage1(ver3)/manifest.parquet\n",
            "- 003. United Health: found=5, processed=0, skipped=5, manifest_rows=5\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/003. United Health/esg_stage1(ver3)/manifest.parquet\n",
            "- 004. Apple: found=5, processed=0, skipped=5, manifest_rows=5\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/004. Apple/esg_stage1(ver3)/manifest.parquet\n",
            "- 005. CVS Health: found=5, processed=0, skipped=5, manifest_rows=5\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/005. CVS Health/esg_stage1(ver3)/manifest.parquet\n",
            "- 006. ExxonMobil: found=6, processed=0, skipped=6, manifest_rows=6\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/006. ExxonMobil/esg_stage1(ver3)/manifest.parquet\n",
            "- 007. Alphabet: found=6, processed=0, skipped=6, manifest_rows=6\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/007. Alphabet/esg_stage1(ver3)/manifest.parquet\n",
            "- 008. Berkshire Hathaway: found=5, processed=0, skipped=5, manifest_rows=5\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/008. Berkshire Hathaway/esg_stage1(ver3)/manifest.parquet\n",
            "- 009. McKesson: found=6, processed=0, skipped=6, manifest_rows=6\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/009. McKesson/esg_stage1(ver3)/manifest.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STAGE 1 WITH TEXT IMPROVEMENT (PAGE NUMBER)"
      ],
      "metadata": {
        "id": "IOvvKU4X99lD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Stage 1 — Text Extraction (improved)\n",
        "# Native PDF via pdftotext(-layout) → fallback PyPDF → OCR (layout-aware)\n",
        "# - Header/footer stripping via page-line frequency\n",
        "# - ToC page/line detection and suppression\n",
        "# - Fix broken hyphens & wrapped lines\n",
        "# - Bullet/leader normalization\n",
        "# - Safe sentence joining before NLTK sent_tokenize\n",
        "# ===============================\n",
        "!apt-get -qq update && apt-get -qq install -y poppler-utils tesseract-ocr >/dev/null\n",
        "!pip -q install pypdf pdf2image pytesseract nltk pandas pyarrow tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os, re, io, subprocess, hashlib, unicodedata, statistics, string\n",
        "from collections import defaultdict, Counter\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from pypdf import PdfReader\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import nltk; nltk.download('punkt', quiet=True)\n",
        "import nltk; nltk.download('punkt_tab', quiet=True)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "IN_DIR            = \"/content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/\"\n",
        "SKIP_EXISTING     = True\n",
        "NATIVE_MIN_CHARS  = 120\n",
        "OCR_DPI           = 300\n",
        "SENT_MINLEN       = 25\n",
        "\n",
        "# Cleanup toggles\n",
        "DROP_TOC_PAGES            = True\n",
        "DROP_TOC_LINES_EVERYWHERE = True\n",
        "STRIP_HEADERS_FOOTERS     = True\n",
        "HEADER_FOOTER_MIN_PAGES   = 4     # if a top/bottom line repeats on >= this many pages → drop\n",
        "WRAP_JOIN_WIDTH_CHARS     = 64    # if a short line (no terminal punctuation) followed by lowercase start → join\n",
        "MAX_PAGE_NUMBER           = 400   # cap for small-int detection on ToC\n",
        "\n",
        "# ---------- Regex & helpers ----------\n",
        "YEAR_RE   = re.compile(r'_(19|20)\\d{2}(?=\\.pdf$)', re.I)\n",
        "PAGINATION= re.compile(r\"\\b(page|pp\\.|p\\.|appendix|annex|exhibit|figure|table|chapter|section|see page)\\b\", re.I)\n",
        "TOC_LEADER= re.compile(r\"\\.{3,}\\s*\\d{1,4}$\")\n",
        "BARE_NUM  = re.compile(r\"^\\s*\\d{1,3}\\s*$\")\n",
        "DATE_ONLY = re.compile(r\"^\\s*(?:FY\\s*\\d{4}|(19|20)\\d{2}|Q[1-4])\\s*$\", re.I)\n",
        "SHORT_HEAD= re.compile(r\"^(?:contents|table of contents|index)$\", re.I)\n",
        "SMALL_INT = re.compile(r\"\\b(?:[1-9]\\d{0,2})\\b\")  # 1..999\n",
        "END_PUNCT = re.compile(r\"[.!?…]\\s*$\")\n",
        "BULLET_CH = \"•●▪◦‣∙·\"\n",
        "BULLET_RE = re.compile(rf\"\\s*[{re.escape(BULLET_CH)}]\\s*\")\n",
        "SOFT_HYPH = re.compile(r\"(\\w)-\\s*\\n\\s*(\\w)\")  # hyphen wrap across newline\n",
        "\n",
        "def extract_year(p: Path):\n",
        "    m = YEAR_RE.search(p.name); return int(m.group(0)[1:]) if m else None\n",
        "\n",
        "def extract_type(p: Path):\n",
        "    n = p.name.upper()\n",
        "    if \"_AR_\" in n: return \"AR\"\n",
        "    if \"_10K_\" in n: return \"10K\"\n",
        "    return \"ESG\"\n",
        "\n",
        "def sha256_16(p: Path):\n",
        "    h=hashlib.sha256()\n",
        "    with open(p,'rb') as f:\n",
        "        for b in iter(lambda:f.read(1<<20), b''): h.update(b)\n",
        "    return h.hexdigest()[:16]\n",
        "\n",
        "def normalize_ws(text: str) -> str:\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    # keep newlines for page/line logic; collapse excessive spaces\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "def is_toc_like_line(line: str) -> bool:\n",
        "    s = line.strip()\n",
        "    if not s: return False\n",
        "    # multiple title+page-number pairs or many small integers and little punctuation\n",
        "    pairs = len(re.findall(r\"[A-Za-z][\\w’&/ -]{2,}\\s{1,}\\d{1,3}(?:\\s{2,}|\\s*$)\", s))\n",
        "    small_ints = [int(x) for x in SMALL_INT.findall(s) if int(x) <= MAX_PAGE_NUMBER]\n",
        "    punct = sum(ch in \",;:.!?()\" for ch in s)\n",
        "    # dot leaders classic\n",
        "    if TOC_LEADER.search(s): return True\n",
        "    if pairs >= 3: return True\n",
        "    if len(small_ints) >= 5 and punct <= 1: return True\n",
        "    toc_keywords = (\"contents\",\"data tables\",\"endnotes\",\"assurance\",\"disclaimer\",\"overview\",\"goals summary\")\n",
        "    if any(k in s.lower() for k in toc_keywords) and (pairs >= 2 or len(small_ints) >= 4):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def is_toc_page(text: str) -> bool:\n",
        "    lines = [ln for ln in text.splitlines() if ln.strip()]\n",
        "    if not lines: return False\n",
        "    score = sum(is_toc_like_line(ln) for ln in lines)\n",
        "    return score >= max(5, int(0.3*len(lines)))  # many toc-like lines\n",
        "\n",
        "def fingerprint_header_footer_per_page(pages: list[str]):\n",
        "    # header = first non-empty line; footer = last non-empty line\n",
        "    headers, footers = [], []\n",
        "    for pg in pages:\n",
        "        ls = [l.strip() for l in pg.splitlines() if l.strip()]\n",
        "        if not ls:\n",
        "            headers.append(\"\"); footers.append(\"\"); continue\n",
        "        headers.append(ls[0]); footers.append(ls[-1])\n",
        "    return headers, footers\n",
        "\n",
        "def compute_drop_lines(headers, footers, min_pages=HEADER_FOOTER_MIN_PAGES):\n",
        "    hc = Counter(headers); fc = Counter(footers)\n",
        "    drop = set()\n",
        "    for line, cnt in hc.items():\n",
        "        if line and cnt >= min_pages: drop.add(line)\n",
        "    for line, cnt in fc.items():\n",
        "        if line and cnt >= min_pages: drop.add(line)\n",
        "    return drop\n",
        "\n",
        "def strip_headers_footers(text: str, drop_lines: set[str]) -> str:\n",
        "    if not drop_lines: return text\n",
        "    out = []\n",
        "    for ln in text.splitlines():\n",
        "        if ln.strip() in drop_lines: continue\n",
        "        out.append(ln)\n",
        "    return \"\\n\".join(out)\n",
        "\n",
        "def fix_broken_hyphens(text: str) -> str:\n",
        "    # join word-wrap hyphen breaks across newline\n",
        "    text = SOFT_HYPH.sub(r\"\\1\\2\", text)\n",
        "    # remove discretionary hyphen (Unicode soft hyphen)\n",
        "    text = text.replace(\"\\u00AD\", \"\")\n",
        "    return text\n",
        "\n",
        "def join_wrapped_lines(text: str) -> str:\n",
        "    # Join lines that are likely wrapped (no end punctuation and next starts lowercase)\n",
        "    lines = text.splitlines()\n",
        "    out = []\n",
        "    i = 0\n",
        "    while i < len(lines):\n",
        "        cur = lines[i].rstrip()\n",
        "        while i+1 < len(lines):\n",
        "            nxt = lines[i+1].lstrip()\n",
        "            if not cur: break\n",
        "            # If current line is short and doesn't end with sentence punctuation, and next is a lowercase/continuation, join.\n",
        "            if (len(cur) < WRAP_JOIN_WIDTH_CHARS and not END_PUNCT.search(cur)\n",
        "                and (nxt[:1].islower() or (nxt and nxt[0] in string.ascii_lowercase))\n",
        "                and not SHORT_HEAD.match(cur)\n",
        "               ):\n",
        "                cur = cur + \" \" + nxt\n",
        "                i += 1\n",
        "            else:\n",
        "                break\n",
        "        out.append(cur)\n",
        "        i += 1\n",
        "    return \"\\n\".join(out)\n",
        "\n",
        "def normalize_bullets_and_leaders(text: str) -> str:\n",
        "    # Turn \"• item\" into \" - item\" and collapse multiple bullets on one line with semicolons\n",
        "    text = BULLET_RE.sub(\" - \", text)\n",
        "    # Replace midline bullet separators with \" ; \" to aid sentence tokenization\n",
        "    text = re.sub(rf\"\\s*[{re.escape(BULLET_CH)}]\\s*\", \" ; \", text)\n",
        "    return text\n",
        "\n",
        "def clean_lines_keep_structure(text: str, drop_toc_pages=True, drop_toc_lines=True, strip_hf=True):\n",
        "    text = normalize_ws(text)\n",
        "    pages = [pg for pg in re.split(r\"\\f|\\x0c\", text)]  # form-feed split if present\n",
        "    if not pages: pages = [text]\n",
        "\n",
        "    # detect headers/footers\n",
        "    drop_set = set()\n",
        "    if strip_hf:\n",
        "        h, f = fingerprint_header_footer_per_page(pages)\n",
        "        drop_set = compute_drop_lines(h, f, HEADER_FOOTER_MIN_PAGES)\n",
        "\n",
        "    new_pages = []\n",
        "    for pg in pages:\n",
        "        if not pg.strip():\n",
        "            new_pages.append(pg);\n",
        "            continue\n",
        "        if drop_toc_pages and is_toc_page(pg):\n",
        "            # keep page for traceability but blank out\n",
        "            new_pages.append(\"\")\n",
        "            continue\n",
        "        # strip headers/footers by line content\n",
        "        if drop_set:\n",
        "            pg = strip_headers_footers(pg, drop_set)\n",
        "        # remove obvious non-content lines\n",
        "        kept = []\n",
        "        for ln in pg.splitlines():\n",
        "            s = ln.strip()\n",
        "            if not s:\n",
        "                kept.append(ln);\n",
        "                continue\n",
        "            if BARE_NUM.match(s):\n",
        "                continue\n",
        "            if TOC_LEADER.search(s):\n",
        "                continue\n",
        "            if SHORT_HEAD.match(s):\n",
        "                continue\n",
        "            if PAGINATION.search(s) and len(s.split()) <= 6:\n",
        "                continue\n",
        "            if DATE_ONLY.match(s):\n",
        "                continue\n",
        "            if drop_toc_lines and is_toc_like_line(s):\n",
        "                continue\n",
        "            kept.append(ln)\n",
        "        page_txt = \"\\n\".join(kept)\n",
        "        new_pages.append(page_txt)\n",
        "\n",
        "    cleaned = \"\\n\".join(new_pages)\n",
        "    cleaned = fix_broken_hyphens(cleaned)\n",
        "    cleaned = join_wrapped_lines(cleaned)\n",
        "    cleaned = normalize_bullets_and_leaders(cleaned)\n",
        "    # Collapse >2 consecutive newlines but keep paragraph boundaries\n",
        "    cleaned = re.sub(r\"\\n{3,}\", \"\\n\\n\", cleaned)\n",
        "    return cleaned.strip()\n",
        "\n",
        "def to_sentences(txt: str):\n",
        "    txt = clean_lines_keep_structure(txt,\n",
        "                                     drop_toc_pages=DROP_TOC_PAGES,\n",
        "                                     drop_toc_lines=DROP_TOC_LINES_EVERYWHERE,\n",
        "                                     strip_hf=STRIP_HEADERS_FOOTERS)\n",
        "    # Help tokenizer by ensuring paragraphs have terminal punctuation:\n",
        "    txt = re.sub(r\"(?<![.!?;])\\n(?=[A-Z])\", \". \", txt)  # paragraph starts with cap\n",
        "    sents = [s.strip() for s in sent_tokenize(txt)]\n",
        "    sents = [s for s in sents if len(s) >= SENT_MINLEN]\n",
        "    return sents\n",
        "\n",
        "# ---------- Native extraction ----------\n",
        "def native_text_pdftotext(pdf: Path) -> str:\n",
        "    try:\n",
        "        # -layout preserves columns; -q quiet; returns bytes\n",
        "        out = subprocess.check_output(\n",
        "            [\"pdftotext\", \"-layout\", \"-q\", str(pdf), \"-\"],\n",
        "            stderr=subprocess.DEVNULL\n",
        "        )\n",
        "        return out.decode(\"utf-8\", errors=\"ignore\")\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def native_text_pypdf(pdf: Path) -> str:\n",
        "    try:\n",
        "        pages = PdfReader(str(pdf)).pages\n",
        "        parts = [(pg.extract_text() or \"\") for pg in pages]\n",
        "        return \"\\n\\f\\n\".join(parts)  # keep page separators\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def native_text(pdf: Path) -> str:\n",
        "    txt = native_text_pdftotext(pdf)\n",
        "    if len(txt.strip()) < NATIVE_MIN_CHARS:\n",
        "        txt = native_text_pypdf(pdf)\n",
        "    return txt\n",
        "\n",
        "# ---------- OCR extraction ----------\n",
        "def ocr_fulldoc(pdf: Path) -> str:\n",
        "    try:\n",
        "        imgs = convert_from_path(str(pdf), dpi=OCR_DPI)\n",
        "        txts = []\n",
        "        for im in imgs:\n",
        "            t = pytesseract.image_to_string(\n",
        "                im, lang=\"eng\", config=\"--oem 1 --psm 6\"\n",
        "            )\n",
        "            txts.append(t or \"\")\n",
        "        return \"\\n\\f\\n\".join(txts)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# ---------- Discover PDFs & group by company folder ----------\n",
        "IN_ROOT = Path(IN_DIR)\n",
        "all_pdfs = sorted(IN_ROOT.rglob(\"*.pdf\"))\n",
        "groups = defaultdict(list)\n",
        "for pdf in all_pdfs:\n",
        "    groups[pdf.parent].append(pdf)\n",
        "\n",
        "total_processed = 0\n",
        "total_skipped = 0\n",
        "per_folder_summaries = []\n",
        "\n",
        "# ---------- Process per folder ----------\n",
        "for folder, pdfs in groups.items():\n",
        "    out_root  = folder / \"esg_stage1(ver4)\"\n",
        "    texts_dir = out_root / \"texts\"\n",
        "    sents_dir = out_root / \"sentences\"\n",
        "    texts_dir.mkdir(parents=True, exist_ok=True)\n",
        "    sents_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    manifest_path = out_root / \"manifest.parquet\"\n",
        "    existing = pd.read_parquet(manifest_path) if (SKIP_EXISTING and manifest_path.exists()) else pd.DataFrame()\n",
        "\n",
        "    rows = []\n",
        "    skipped_here = 0\n",
        "    processed_here = 0\n",
        "\n",
        "    for pdf in tqdm(pdfs, desc=f\"Processing PDFs in {folder.name}\", leave=False):\n",
        "        filename = pdf.name\n",
        "        if SKIP_EXISTING and not existing.empty and filename in existing[\"filename\"].values:\n",
        "            skipped_here += 1\n",
        "            continue\n",
        "\n",
        "        year = extract_year(pdf); rtype = extract_type(pdf); hid = sha256_16(pdf)\n",
        "        text_path = texts_dir / f\"{hid}.txt\"\n",
        "        sent_path = sents_dir / f\"{hid}.parquet\"\n",
        "\n",
        "        # 1) Native (pdftotext → PyPDF)\n",
        "        raw = native_text(pdf)\n",
        "        # 2) Decide OCR if too short\n",
        "        if raw and len(raw) >= NATIVE_MIN_CHARS:\n",
        "            status = \"ok_native\"\n",
        "            txt_to_save = raw\n",
        "        else:\n",
        "            raw = ocr_fulldoc(pdf)\n",
        "            status = \"ok_ocr\" if raw and len(raw) >= NATIVE_MIN_CHARS else \"error_empty\"\n",
        "            txt_to_save = raw\n",
        "\n",
        "        # Save raw (normalized a bit) and sentences (post-clean)\n",
        "        text_path.write_text(normalize_ws(txt_to_save) if txt_to_save else \"\", encoding=\"utf-8\")\n",
        "        sents = to_sentences(txt_to_save) if txt_to_save else []\n",
        "        pd.DataFrame({\"text\": sents}).to_parquet(sent_path, index=False)\n",
        "\n",
        "        rows.append({\n",
        "            \"sha256_16\": hid,\n",
        "            \"filename\": filename,\n",
        "            \"pdf_path\": str(pdf),\n",
        "            \"text_path\": str(text_path),\n",
        "            \"sentences_path\": str(sent_path),\n",
        "            \"status\": status,\n",
        "            \"doc_year\": year,\n",
        "            \"report_type\": rtype,\n",
        "            \"dropped_toc_pages\": DROP_TOC_PAGES,\n",
        "            \"dropped_toc_lines\": DROP_TOC_LINES_EVERYWHERE,\n",
        "            \"strip_headers_footers\": STRIP_HEADERS_FOOTERS\n",
        "        })\n",
        "        processed_here += 1\n",
        "\n",
        "    # Merge+save manifest\n",
        "    if rows:\n",
        "        manifest = (pd.concat([existing, pd.DataFrame(rows)], ignore_index=True)\n",
        "                    if not existing.empty else pd.DataFrame(rows))\n",
        "        manifest = manifest.drop_duplicates(subset=[\"filename\"], keep=\"last\")\n",
        "        manifest.to_parquet(manifest_path, index=False)\n",
        "    else:\n",
        "        manifest = existing if not existing.empty else pd.DataFrame(columns=[\n",
        "            \"sha256_16\",\"filename\",\"pdf_path\",\"text_path\",\"sentences_path\",\"status\",\n",
        "            \"doc_year\",\"report_type\",\"dropped_toc_pages\",\"dropped_toc_lines\",\"strip_headers_footers\"\n",
        "        ])\n",
        "\n",
        "    per_folder_summaries.append({\n",
        "        \"folder\": str(folder),\n",
        "        \"pdfs_found\": len(pdfs),\n",
        "        \"processed_now\": processed_here,\n",
        "        \"skipped_existing\": skipped_here,\n",
        "        \"manifest_path\": str(manifest_path),\n",
        "        \"manifest_rows\": len(manifest)\n",
        "    })\n",
        "    total_processed += processed_here\n",
        "    total_skipped += skipped_here\n",
        "\n",
        "# ---------- Result ----------\n",
        "print(\"\\n✅ Done.\")\n",
        "print(f\"Folders: {len(groups)}  |  PDFs total: {len(all_pdfs)}\")\n",
        "print(f\"Processed now: {total_processed}  |  Skipped (existing): {total_skipped}\\n\")\n",
        "for s in per_folder_summaries[:10]:\n",
        "    print(f\"- {Path(s['folder']).name}: found={s['pdfs_found']}, processed={s['processed_now']}, \"\n",
        "          f\"skipped={s['skipped_existing']}, manifest_rows={s['manifest_rows']}\")\n",
        "    print(f\"  → {s['manifest_path']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Twfz5vpI83gS",
        "outputId": "049a684c-bbdb-48a9-de64-e19d97fec501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing PDFs in 174. 3M:  67%|██████▋   | 4/6 [00:19<00:09,  4.85s/it]WARNING:pypdf._reader:incorrect startxref pointer(3)\n",
            "WARNING:pypdf._reader:parsing for Object Streams\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Done.\n",
            "Folders: 493  |  PDFs total: 2278\n",
            "Processed now: 2278  |  Skipped (existing): 0\n",
            "\n",
            "- 000. Mini Trial: found=5, processed=5, skipped=0, manifest_rows=5\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/000. Mini Trial/esg_stage1(ver4)/manifest.parquet\n",
            "- 001. Walmart: found=6, processed=6, skipped=0, manifest_rows=6\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/001. Walmart/esg_stage1(ver4)/manifest.parquet\n",
            "- 002. Amazon: found=5, processed=5, skipped=0, manifest_rows=5\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/002. Amazon/esg_stage1(ver4)/manifest.parquet\n",
            "- 003. United Health: found=5, processed=5, skipped=0, manifest_rows=5\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/003. United Health/esg_stage1(ver4)/manifest.parquet\n",
            "- 004. Apple: found=5, processed=5, skipped=0, manifest_rows=5\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/004. Apple/esg_stage1(ver4)/manifest.parquet\n",
            "- 005. CVS Health: found=5, processed=5, skipped=0, manifest_rows=5\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/005. CVS Health/esg_stage1(ver4)/manifest.parquet\n",
            "- 006. ExxonMobil: found=6, processed=6, skipped=0, manifest_rows=6\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/006. ExxonMobil/esg_stage1(ver4)/manifest.parquet\n",
            "- 007. Alphabet: found=6, processed=6, skipped=0, manifest_rows=6\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/007. Alphabet/esg_stage1(ver4)/manifest.parquet\n",
            "- 008. Berkshire Hathaway: found=5, processed=5, skipped=0, manifest_rows=5\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/008. Berkshire Hathaway/esg_stage1(ver4)/manifest.parquet\n",
            "- 009. McKesson: found=6, processed=6, skipped=0, manifest_rows=6\n",
            "  → /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/009. McKesson/esg_stage1(ver4)/manifest.parquet\n"
          ]
        }
      ]
    }
  ]
}