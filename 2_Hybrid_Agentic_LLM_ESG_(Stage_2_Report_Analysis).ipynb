{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Stage 2 (Hybrid)\n",
        "# ===============================\n",
        "\n",
        "\n",
        "!pip -q install pandas numpy nltk pyarrow flashtext openai\n",
        "\n",
        "from google.colab import drive; drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os, re, json, unicodedata, hashlib, math, time\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, List, Set\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk; nltk.download(\"punkt\", quiet=True)\n",
        "import nltk; nltk.download(\"punkt_tab\", quiet=True)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from flashtext import KeywordProcessor\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# ---------- Config ----------\n",
        "COMPANIES_ROOT    = \"<<Google Drive Folder>>/FORTUNE 500/\"\n",
        "UNIFIED_FILE      = \"/unified_keywords.csv\"  # pillar,canonical_concept,keyword\n",
        "COMPANY_SIC_FILE  = \"Fortune500_with_SIC.csv\"\n",
        "CACHE_FILE        = \"<<Google Drive Folder>>/llm_quant_cache.parquet\"  # persistent cache for quant/gran results\n",
        "\n",
        "# Folder subset; \"\" = all (e.g., \"001-030,050-200,201,205-208\")\n",
        "FOLDER_SELECT  = \"001-500\"\n",
        "\n",
        "# Filters; [] = none\n",
        "YEAR_LIST   = []             # e.g., [2022, 2023]\n",
        "REPORT_TYPE = []             # e.g., [\"AR\",\"10K\"]\n",
        "\n",
        "# ✅ Resumable: if True, skip any doc whose Stage-2 JSON already exists\n",
        "ONLY_MISSING = True\n",
        "\n",
        "# LLM settings (only used for Quant/Granularity)\n",
        "LLM_MODEL          = \"gpt-3.5-turbo\"\n",
        "LLM_TEMPERATURE    = 0\n",
        "LLM_MAX_OUT_TOKENS = 3_000\n",
        "PROMPT_VERSION     = \"v1.0\"           # bump to invalidate cache safely\n",
        "\n",
        "# Safety caps\n",
        "MAX_SENT_PER_DOC   = 5000\n",
        "MIN_SENT_LEN       = 25\n",
        "MAX_SPAN_CHARS     = 320\n",
        "BATCH_MAX_ITEMS    = 80\n",
        "\n",
        "# ---------- OpenAI ----------\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OpenAPI\")\n",
        "client = OpenAI()\n",
        "\n",
        "# ---------- Divisions (fallback) ----------\n",
        "DIVISIONS = [\n",
        "    (\"A\",\"Agriculture, Forestry & Fishing\"), (\"B\",\"Mining\"), (\"C\",\"Construction\"),\n",
        "    (\"D\",\"Manufacturing\"), (\"E\",\"Transportation & Public Utilities\"),\n",
        "    (\"F\",\"Wholesale Trade\"), (\"G\",\"Retail Trade\"),\n",
        "    (\"H\",\"Finance, Insurance & Real Estate\"), (\"I\",\"Services\"), (\"J\",\"Public Administration\"),\n",
        "]\n",
        "DIV_MAP = {c:n for c,n in DIVISIONS}\n",
        "\n",
        "# ---------- Regex / detectors ----------\n",
        "FOOTNOTE_LEAD = re.compile(r\"^\\s*(?:\\(?\\d+\\)?|\\[\\d+\\]|[¹²³⁴⁵⁶⁷⁸⁹])\\s+\")\n",
        "TOC_LINE      = re.compile(r\"\\.{3,}\\s*\\d{1,4}$\")\n",
        "PAGINATION    = re.compile(r\"\\b(page|pp\\.|p\\.|appendix|annex|exhibit|figure|table|chapter|section|see page)\\b\", re.I)\n",
        "DATE_HINT     = re.compile(r\"\\b(19|20)\\d{2}\\b|\\b(?:jan|feb|mar|apr|may|jun|jul|aug|sept?|oct|nov|dec|fy|q[1-4])\\b\", re.I)\n",
        "NUM_PAT       = re.compile(r\"[+-]?\\d{1,3}(?:[,\\s]\\d{3})*(?:\\.\\d+)?|[+-]?\\d+(?:\\.\\d+)?\")\n",
        "METRIC_HINT   = re.compile(\n",
        "    r\"\"\"(?ix)\n",
        "    (tco2e?|co2e?|ghg|emission|scope\\s*[123]|carbon|methane|ch4|\n",
        "     kwh|mwh|gwh|wh|gj|mj|energy|electricity|renewable|\n",
        "     m3|m2|m²|lit(?:re|er)s?|water|withdrawals?|discharge|\n",
        "     kg|t|tonnes?|metric\\s*tons?|waste|recycled|landfill|\n",
        "     incident[s]?|injur(?:y|ies)|fatalit(?:y|ies)|spills?|leaks?|\n",
        "     fine[s]?|penalt(?:y|ies)|hours|training|turnover|absenteeism|\n",
        "     diversity|board|pay|remuneration|audit|briber|ethic|\n",
        "     usd|aud|eur|cad|sgd|yen|cny|inr|zar|mxn|brl|sek|nok|dkk|\n",
        "     us\\$|a\\$|\\$|£|¥|€)\n",
        "    \"\"\"\n",
        ")\n",
        "WINDOW_METRIC = re.compile(r\"(?:^|\\b)(?:rate|intensity|emissions?|energy|water|waste|injur|incident|fine|penalt|spent|invested|allocated|revenue|cost|capex)\\b\", re.I)\n",
        "SPECIFIC_PAT  = re.compile(\n",
        "    r\"\"\"(?ix)\\b(from|to|versus|vs\\.?|compared|baseline|yoy|year[-\\s]?on[-\\s]?year|\n",
        "       reduc(?:e|ed)|increas(?:e|ed)|improv(?:e|ed)|decreas(?:e|ed)|\n",
        "       drop(?:ped)?|rose|grew|cut|achiev(?:e|ed))\\b\"\"\"\n",
        ")\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    s = unicodedata.normalize(\"NFKC\", s.lower())\n",
        "    s = s.replace(\"_\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "def clean_for_quant(s: str) -> str:\n",
        "    s = FOOTNOTE_LEAD.sub(\"\", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# ---------- Keywords ----------\n",
        "def load_concepts(path=UNIFIED_FILE):\n",
        "    try:\n",
        "        df = pd.read_csv(path, usecols=[\"pillar\",\"canonical_concept\",\"keyword\"]).dropna()\n",
        "        df[\"pillar\"] = df[\"pillar\"].astype(str).str.strip()\n",
        "        df[\"canonical_concept\"] = df[\"canonical_concept\"].astype(str).str.strip()\n",
        "        df[\"keyword\"] = df[\"keyword\"].astype(str).str.strip()\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"[ERROR] Keyword file not found at: {path}. Please ensure Google Drive is mounted and the file exists.\")\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed reading keyword CSV '{path}': {e}. Please check file encoding and integrity. Ensure Google Drive is mounted.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "def build_keyword_processor(df_keywords: pd.DataFrame) -> Tuple[KeywordProcessor, Dict[str, Tuple[str,str,str]], Dict[str, Dict[str, Set[str]]]]:\n",
        "    kp = KeywordProcessor(case_sensitive=False)\n",
        "    keymap = {}\n",
        "    concepts_index: Dict[str, Dict[str, Set[str]]] = {}\n",
        "\n",
        "    for _, r in df_keywords.iterrows():\n",
        "        pillar = str(r[\"pillar\"]).strip()\n",
        "        concept = str(r[\"canonical_concept\"]).strip()\n",
        "        kw = str(r[\"keyword\"]).strip()\n",
        "        if not pillar or not concept or not kw:\n",
        "            continue\n",
        "\n",
        "        base = normalize_text(kw)\n",
        "        variants: Set[str] = set([base])\n",
        "        variants.add(base.replace(\"-\", \" \"))\n",
        "        variants.add(base.replace(\" \", \"-\"))\n",
        "        variants.add(base.replace(\" \", \"\"))  # occasional OCR joining\n",
        "        if not base.endswith(\"s\") and len(base.split())==1 and len(base)>=4:\n",
        "            variants.add(base + \"s\")\n",
        "\n",
        "        concepts_index.setdefault(pillar, {}).setdefault(concept, set()).add(base)\n",
        "\n",
        "        for v in variants:\n",
        "            if not v: continue\n",
        "            kp.add_keyword(v, v)\n",
        "            keymap[v] = (pillar, concept, kw)\n",
        "\n",
        "    return kp, keymap, concepts_index\n",
        "\n",
        "def empty_concept_state():\n",
        "    return {\n",
        "        \"count\":0, \"examples\":[],\n",
        "        \"qual_examples\":[], \"qgen_examples\":[], \"qspec_examples\":[],\n",
        "        \"n_qual\":0,\"n_qgen\":0,\"n_qspec\":0,\n",
        "        \"labels\":set(), \"final_label\":\"None\"\n",
        "    }\n",
        "\n",
        "# ---------- Stage1 readers ----------\n",
        "def _resolve(path_str: str, base_dir: Path) -> str:\n",
        "    if not path_str or not isinstance(path_str, str):\n",
        "        return \"\"\n",
        "    p = Path(path_str)\n",
        "    if p.is_absolute():\n",
        "        return str(p)\n",
        "    return str((base_dir / p).resolve())\n",
        "\n",
        "def read_sentences(row):\n",
        "    base_dir = Path(row.get(\"__base_dir__\", \"\"))  # per-company stage dir (esg_stage or esg_stage1)\n",
        "    s = row.get(\"sentences_path\"); t = row.get(\"text_path\")\n",
        "    sents=[]\n",
        "    if s and isinstance(s,str):\n",
        "        s_abs = _resolve(s, base_dir)\n",
        "        if os.path.exists(s_abs):\n",
        "            try:\n",
        "                df = pd.read_parquet(s_abs, columns=[\"text\"])\n",
        "                sents = [str(x).strip() for x in df[\"text\"].tolist()]\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] read_parquet failed, fallback to text_path. {e}\")\n",
        "    if not sents and t and isinstance(t,str):\n",
        "        t_abs = _resolve(t, base_dir)\n",
        "        if os.path.exists(t_abs):\n",
        "            txt=open(t_abs,\"r\",encoding=\"utf-8\").read()\n",
        "            txt=re.sub(r\"\\s+\",\" \",txt).strip()\n",
        "            sents=[x.strip() for x in sent_tokenize(txt)]\n",
        "    sents=[x for x in sents if len(x)>=MIN_SENT_LEN]\n",
        "    return sents[:MAX_SENT_PER_DOC] if MAX_SENT_PER_DOC else sents\n",
        "\n",
        "# ---------- Company → SIC (from Fortune500_with_SIC.csv) ----------\n",
        "# Name normaliser to make CSV names and filename prefixes comparable\n",
        "_CORP_SUFFIXES = r\"(incorporated|inc|corp|corporation|co|company|ltd|llc|plc|holdings?|group|limited)\"\n",
        "_CORP_SUFFIX_RE = re.compile(rf\"\\b{_CORP_SUFFIXES}\\.?$\", re.I)\n",
        "\n",
        "def normalize_company_name(name: str) -> str:\n",
        "    s = normalize_text(name)\n",
        "    s = re.sub(r\"[^\\w\\s]\", \" \", s)            # remove punctuation\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    # remove trailing corporate suffix (one pass)\n",
        "    s = _CORP_SUFFIX_RE.sub(\"\", s).strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def load_company_sic_map(path: str) -> Dict[str, Tuple[str,str]]:\n",
        "    \"\"\"\n",
        "    CSV columns: Company, SIC Division Code, SIC Division Name\n",
        "    \"\"\"\n",
        "    m = {}\n",
        "    if not path or not os.path.exists(path):\n",
        "        print(f\"[WARN] SIC file not found at: {path}. Will fallback to Services (I).\")\n",
        "        return m\n",
        "    try:\n",
        "        df = pd.read_csv(path, dtype=str, encoding='latin1').fillna(\"\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Failed reading SIC CSV '{path}': {e}. Fallback to empty map.\")\n",
        "        return m\n",
        "\n",
        "    required = {\"Company\",\"SIC Division Code\",\"SIC Division Name\"}\n",
        "    missing = required - set(df.columns)\n",
        "    if missing:\n",
        "        print(f\"[WARN] SIC CSV missing columns: {missing}. Fallback to empty map.\")\n",
        "        return m\n",
        "\n",
        "    cnt = 0\n",
        "    for _, r in df.iterrows():\n",
        "        company = normalize_company_name(r[\"Company\"])\n",
        "        code = str(r[\"SIC Division Code\"]).strip().upper()\n",
        "        name = str(r[\"SIC Division Name\"]).strip()\n",
        "        if company and code and name:\n",
        "            m[company] = (code, name); cnt += 1\n",
        "    print(f\"[SIC] Loaded {cnt} company→SIC mappings from {path}\")\n",
        "    return m\n",
        "\n",
        "COMPANY_SIC = load_company_sic_map(COMPANY_SIC_FILE)\n",
        "\n",
        "def company_from_filename(name: str) -> str:\n",
        "    if not name: return \"\"\n",
        "    stem = Path(name).stem\n",
        "    return stem.split(\"_\")[0].strip()\n",
        "\n",
        "def get_company_sic_by_name(filename: str) -> Tuple[str,str]:\n",
        "    cname_raw = company_from_filename(filename)\n",
        "    cname = normalize_company_name(cname_raw)\n",
        "    if cname in COMPANY_SIC:\n",
        "        return COMPANY_SIC[cname]\n",
        "    # Try a second-chance variant: remove \"the\" prefix or extra spaces\n",
        "    cname2 = re.sub(r\"^\\bthe\\b\\s+\", \"\", cname).strip()\n",
        "    if cname2 in COMPANY_SIC:\n",
        "        return COMPANY_SIC[cname2]\n",
        "    # Fallback\n",
        "    return (\"I\", \"Services\")\n",
        "\n",
        "# ---------- LLM cache ----------\n",
        "def load_cache(path: str) -> Dict[str, Dict]:\n",
        "    if os.path.exists(path):\n",
        "        try:\n",
        "            df = pd.read_parquet(path)\n",
        "            out = {}\n",
        "            for _, r in df.iterrows():\n",
        "                out[r[\"key\"]] = {\"quant\": bool(r[\"quant\"]), \"granularity\": r[\"granularity\"]}\n",
        "            return out\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Failed to read cache, starting fresh: {e}\")\n",
        "    return {}\n",
        "\n",
        "def save_cache(cache: Dict[str, Dict], path: str):\n",
        "    if not cache:\n",
        "        return\n",
        "    rows = [{\"key\":k, \"quant\":v.get(\"quant\", False), \"granularity\": v.get(\"granularity\",\"\")} for k,v in cache.items()]\n",
        "    df = pd.DataFrame(rows).drop_duplicates(\"key\")\n",
        "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_parquet(path, index=False)\n",
        "\n",
        "LLM_CACHE = load_cache(CACHE_FILE)\n",
        "\n",
        "def sent_key_for_cache(text: str) -> str:\n",
        "    base = normalize_text(text)[:MAX_SPAN_CHARS]\n",
        "    h = hashlib.sha1((PROMPT_VERSION + \"|\" + LLM_MODEL + \"|\" + base).encode(\"utf-8\")).hexdigest()\n",
        "    return h\n",
        "\n",
        "# ---------- LLM triage ----------\n",
        "def triage_quant_candidates(sentences: List[str]) -> Dict[str, str]:\n",
        "    cand = {}\n",
        "    for idx, s in enumerate(sentences):\n",
        "        s_clean = clean_for_quant(s)\n",
        "        s_norm  = normalize_text(s_clean)\n",
        "\n",
        "        if TOC_LINE.search(s_norm):\n",
        "            continue\n",
        "        if PAGINATION.search(s_norm) and not METRIC_HINT.search(s_norm):\n",
        "            continue\n",
        "\n",
        "        has_num = bool(NUM_PAT.search(s_norm))\n",
        "        if not has_num:\n",
        "            continue\n",
        "        if DATE_HINT.search(s_norm) and not (METRIC_HINT.search(s_norm) or WINDOW_METRIC.search(s_norm)):\n",
        "            continue\n",
        "\n",
        "        # compact span\n",
        "        m = NUM_PAT.search(s_clean)\n",
        "        span = s_clean\n",
        "        if m:\n",
        "            start = max(0, m.start() - 140)\n",
        "            end   = min(len(s_clean), m.end() + 140)\n",
        "            span  = s_clean[start:end]\n",
        "        span = span[:MAX_SPAN_CHARS].strip()\n",
        "        if not span:\n",
        "            continue\n",
        "        sid = f\"S{idx}\"\n",
        "        cand[sid] = span\n",
        "    return cand\n",
        "\n",
        "# ---------- LLM batches for Quant & Granularity ----------\n",
        "def llm_classify_quant_gran(batches: List[Dict[str,str]]) -> Dict[str, Dict[str, str]]:\n",
        "    results = {}\n",
        "\n",
        "    def build_prompt(block: Dict[str,str]) -> str:\n",
        "        lines = [f\"{k}: {v}\" for k, v in block.items()]\n",
        "        items = \"\\n\".join(lines)\n",
        "        instr = (\n",
        "            \"For each line, classify if it's Quantitative and its Granularity.\\n\"\n",
        "            \"Rules:\\n\"\n",
        "            \"- Quantitative = contains a numeric value tied to a non-date metric (%, tCO2e, GWh, $, incident rate, hours, etc.). \"\n",
        "            \"Dates/quarters/pages alone are NOT quantitative.\\n\"\n",
        "            \"- Granularity = 'Specific' if it states change/comparison/baseline (e.g., 'from X to Y', 'vs', YoY, 'decreased by %'); \"\n",
        "            \"'General' for a single level/target without explicit comparison.\\n\"\n",
        "            \"Return ONLY a JSON array of objects with fields: id, quant (true/false), and granularity ('General'|'Specific' or omit if quant=false).\\n\"\n",
        "            \"Examples output: \"\n",
        "            \"[{\\\"id\\\":\\\"S1\\\",\\\"quant\\\":true,\\\"granularity\\\":\\\"General\\\"},{\\\"id\\\":\\\"S2\\\",\\\"quant\\\":true,\\\"granularity\\\":\\\"Specific\\\"}]\"\n",
        "        )\n",
        "        return instr + \"\\n\\n\" + items\n",
        "\n",
        "    # Build pool & apply cache\n",
        "    all_items = {}\n",
        "    for block in batches:\n",
        "        for sid, span in block.items():\n",
        "            ckey = sent_key_for_cache(span)\n",
        "            all_items[sid] = (ckey, span)\n",
        "\n",
        "    # Prefill from cache\n",
        "    for sid, (ckey, _) in all_items.items():\n",
        "        if ckey in LLM_CACHE:\n",
        "            results[sid] = {\"quant\": bool(LLM_CACHE[ckey][\"quant\"]), \"granularity\": LLM_CACHE[ckey][\"granularity\"]}\n",
        "\n",
        "    # Remaining blocks\n",
        "    remaining_blocks = []\n",
        "    cur = {}\n",
        "    for sid, (ckey, span) in all_items.items():\n",
        "        if sid in results:\n",
        "            continue\n",
        "        if len(cur) >= BATCH_MAX_ITEMS:\n",
        "            remaining_blocks.append(cur); cur = {}\n",
        "        cur[sid] = span\n",
        "    if cur:\n",
        "        remaining_blocks.append(cur)\n",
        "\n",
        "    # Send to LLM\n",
        "    for block in remaining_blocks:\n",
        "        prompt = build_prompt(block)\n",
        "        tries = 0\n",
        "        while True:\n",
        "            tries += 1\n",
        "            try:\n",
        "                resp = client.chat.completions.create(\n",
        "                    model=LLM_MODEL,\n",
        "                    temperature=LLM_TEMPERATURE,\n",
        "                    messages=[{\"role\":\"user\",\"content\": prompt}],\n",
        "                    max_tokens=LLM_MAX_OUT_TOKENS\n",
        "                )\n",
        "                txt = resp.choices[0].message.content.strip()\n",
        "                data = json.loads(txt)\n",
        "                if not isinstance(data, list):\n",
        "                    data = data.get(\"items\", [])\n",
        "                for obj in data:\n",
        "                    sid = obj.get(\"id\")\n",
        "                    if not sid or sid not in block:\n",
        "                        continue\n",
        "                    quant = bool(obj.get(\"quant\", False))\n",
        "                    gran  = \"\"\n",
        "                    if quant:\n",
        "                        g = str(obj.get(\"granularity\",\"\")).strip().lower()\n",
        "                        gran = \"Specific\" if g.startswith(\"spec\") else \"General\"\n",
        "                    results[sid] = {\"quant\": quant, \"granularity\": gran}\n",
        "                    ckey = sent_key_for_cache(block[sid])\n",
        "                    LLM_CACHE[ckey] = {\"quant\": quant, \"granularity\": gran}\n",
        "                break\n",
        "            except Exception as e:\n",
        "                if tries >= 2:\n",
        "                    # Rules fallback for this block\n",
        "                    for sid, span in block.items():\n",
        "                        s = clean_for_quant(span)\n",
        "                        has_num = bool(NUM_PAT.search(s))\n",
        "                        metricish = bool(METRIC_HINT.search(s) or WINDOW_METRIC.search(s))\n",
        "                        quant = (has_num and metricish) and not (DATE_HINT.search(s) and not METRIC_HINT.search(s))\n",
        "                        gran = \"Specific\" if (quant and SPECIFIC_PAT.search(s)) else (\"General\" if quant else \"\")\n",
        "                        results[sid] = {\"quant\": quant, \"granularity\": gran}\n",
        "                        ckey = sent_key_for_cache(span)\n",
        "                        LLM_CACHE[ckey] = {\"quant\": quant, \"granularity\": gran}\n",
        "                    print(f\"[WARN] LLM batch failed, rules fallback used. Error: {e}\")\n",
        "                    break\n",
        "                time.sleep(1.2)\n",
        "\n",
        "    save_cache(LLM_CACHE, CACHE_FILE)\n",
        "    return results\n",
        "\n",
        "# ---------- Analyze single document (hybrid) ----------\n",
        "def analyze_doc(sentences: List[str], kp: KeywordProcessor, keymap: Dict[str, Tuple[str,str,str]], concepts_index: Dict[str, Dict[str, Set[str]]], precomputed_div=None):\n",
        "    if not sentences:\n",
        "        return {\"sic_division\":None,\"sentences\":0,\"coverage\":{},\"status_by_pillar\":{}}\n",
        "\n",
        "    if precomputed_div:\n",
        "        div_code, div_name = precomputed_div\n",
        "    else:\n",
        "        div_code, div_name = (\"I\",\"Services\")\n",
        "\n",
        "    coverage = {p: { cc: empty_concept_state() for cc in concepts_index[p].keys() } for p in concepts_index.keys()}\n",
        "\n",
        "    fired_by_sentence: Dict[int, List[Tuple[str,str]]] = {}\n",
        "    llm_candidates_order: List[Tuple[int, str]] = []\n",
        "\n",
        "    for i, s in enumerate(sentences):\n",
        "        s_norm = normalize_text(s)\n",
        "        if TOC_LINE.search(s_norm):\n",
        "            continue\n",
        "        if PAGINATION.search(s_norm) and not METRIC_HINT.search(s_norm):\n",
        "            continue\n",
        "\n",
        "        matches = kp.extract_keywords(s_norm)\n",
        "        if not matches:\n",
        "            continue\n",
        "\n",
        "        cset: List[Tuple[str,str]] = []\n",
        "        for mv in matches:\n",
        "            p, cc, _ = keymap.get(mv, (None, None, None))\n",
        "            if p and cc:\n",
        "                cset.append((p, cc))\n",
        "        if not cset:\n",
        "            continue\n",
        "\n",
        "        fired_by_sentence[i] = cset\n",
        "\n",
        "        triaged = triage_quant_candidates([s])\n",
        "        if triaged:\n",
        "            sid = list(triaged.keys())[0]\n",
        "            span = triaged[sid]\n",
        "            llm_candidates_order.append((i, span))\n",
        "\n",
        "    # Batch LLM\n",
        "    blocks, cur = [], {}\n",
        "    for i, span in llm_candidates_order:\n",
        "        sid = f\"S{i}\"\n",
        "        if len(cur) >= BATCH_MAX_ITEMS:\n",
        "            blocks.append(cur); cur = {}\n",
        "        cur[sid] = span\n",
        "    if cur:\n",
        "        blocks.append(cur)\n",
        "\n",
        "    llm_results = llm_classify_quant_gran(blocks) if blocks else {}\n",
        "\n",
        "    # Stitch into coverage\n",
        "    for i, concepts in fired_by_sentence.items():\n",
        "        s = sentences[i]\n",
        "        sid = f\"S{i}\"\n",
        "        quant = False\n",
        "        gran  = \"\"\n",
        "        if sid in llm_results:\n",
        "            quant = bool(llm_results[sid][\"quant\"])\n",
        "            gran  = llm_results[sid][\"granularity\"]\n",
        "\n",
        "        for p, cc in concepts:\n",
        "            if p not in coverage or cc not in coverage[p]:\n",
        "                coverage.setdefault(p, {}).setdefault(cc, empty_concept_state())\n",
        "            st = coverage[p][cc]\n",
        "            st[\"count\"] += 1\n",
        "            if quant:\n",
        "                if gran == \"Specific\":\n",
        "                    st[\"n_qspec\"] += 1\n",
        "                    if len(st[\"qspec_examples\"])<3: st[\"qspec_examples\"].append(s)\n",
        "                    st[\"labels\"].add(\"Quantitative (Specific)\")\n",
        "                else:\n",
        "                    st[\"n_qgen\"] += 1\n",
        "                    if len(st[\"qgen_examples\"])<3: st[\"qgen_examples\"].append(s)\n",
        "                    st[\"labels\"].add(\"Quantitative (General)\")\n",
        "            else:\n",
        "                st[\"n_qual\"] += 1\n",
        "                if len(st[\"qual_examples\"])<3: st[\"qual_examples\"].append(s)\n",
        "                st[\"labels\"].add(\"Qualitative\")\n",
        "            if len(st[\"examples\"])<3 and s not in st[\"examples\"]:\n",
        "                st[\"examples\"].append(s)\n",
        "\n",
        "    for p,d in coverage.items():\n",
        "        for cc,st in d.items():\n",
        "            if   \"Quantitative (Specific)\" in st[\"labels\"]: st[\"final_label\"]=\"Quantitative (Specific)\"\n",
        "            elif \"Quantitative (General)\" in st[\"labels\"]:  st[\"final_label\"]=\"Quantitative (General)\"\n",
        "            elif \"Qualitative\" in st[\"labels\"]:             st[\"final_label\"]=\"Qualitative\"\n",
        "            else:                                           st[\"final_label\"]=\"None\"\n",
        "            st[\"labels\"] = list(st[\"labels\"])\n",
        "\n",
        "    status = {p:{\n",
        "        \"satisfied\":[cc for cc,st in d.items() if st[\"count\"]>0],\n",
        "        \"missing\"  :[cc for cc,st in d.items() if st[\"count\"]==0]\n",
        "    } for p,d in coverage.items()}\n",
        "\n",
        "    return {\n",
        "        \"sic_division\":{\"code\":div_code,\"name\":div_name},\n",
        "        \"sentences\":len(sentences),\n",
        "        \"coverage\":coverage,\n",
        "        \"status_by_pillar\":status\n",
        "    }\n",
        "\n",
        "# ---------- Folder-selection ----------\n",
        "def parse_folder_select(selector: str):\n",
        "    if not selector or not str(selector).strip():\n",
        "        return None\n",
        "    keep = set()\n",
        "    for tok in re.split(r\"[,\\s]+\", selector.strip()):\n",
        "        if not tok: continue\n",
        "        m = re.fullmatch(r\"(\\d{3})-(\\d{3})\", tok)\n",
        "        if m:\n",
        "            a, b = int(m.group(1)), int(m.group(2))\n",
        "            if a <= b: keep.update(range(a, b + 1))\n",
        "            else:      keep.update(range(b, a + 1))\n",
        "            continue\n",
        "        m2 = re.fullmatch(r\"(\\d{3})\", tok)\n",
        "        if m2:\n",
        "            keep.add(int(m2.group(1)))\n",
        "    keep = {x for x in keep if 1 <= x <= 500}\n",
        "    return keep or None\n",
        "\n",
        "def extract_code_from_company_dirname(name: str):\n",
        "    m = re.match(r\"^\\s*(\\d{3})\\b\", name or \"\")\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "# ---------- Manifest discovery ----------\n",
        "companies_root = Path(COMPANIES_ROOT)\n",
        "allowed_codes = parse_folder_select(FOLDER_SELECT)\n",
        "\n",
        "company_dirs = [d for d in companies_root.iterdir() if d.is_dir()]\n",
        "print(f\"Company directories found under COMPANIES_ROOT: {len(company_dirs)}\")\n",
        "\n",
        "frames = []\n",
        "manifests_found = 0\n",
        "checked = 0\n",
        "\n",
        "for cdir in sorted(company_dirs):\n",
        "    checked += 1\n",
        "    code = extract_code_from_company_dirname(cdir.name)\n",
        "    if code is None:\n",
        "        continue\n",
        "    if allowed_codes is not None and code not in allowed_codes:\n",
        "        continue\n",
        "\n",
        "    stage_dir = None\n",
        "    mpath = None\n",
        "    for candidate in (\"esg_stage1(ver4)\", \"esg_stage1(ver3)\"):\n",
        "        cand_dir = cdir / candidate\n",
        "        cand_manifest = cand_dir / \"manifest.parquet\"\n",
        "        if cand_manifest.exists():\n",
        "            stage_dir = cand_dir\n",
        "            mpath = cand_manifest\n",
        "            break\n",
        "    if stage_dir is None:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        df = pd.read_parquet(mpath)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not read manifest for {cdir.name}: {e}\")\n",
        "        continue\n",
        "\n",
        "    df = df[df[\"status\"].isin([\"ok_native\",\"ok_ocr\",\"ok_hybrid\",\"ok\"])].copy()\n",
        "    if df.empty:\n",
        "        continue\n",
        "\n",
        "    df[\"company_code\"] = code\n",
        "    df[\"__base_dir__\"] = str(stage_dir.resolve())\n",
        "    df[\"__company_dir__\"] = str(cdir.resolve())\n",
        "    frames.append(df)\n",
        "    manifests_found += 1\n",
        "\n",
        "print(f\"Companies scanned (after name check & FOLDER_SELECT): {checked}\")\n",
        "print(f\"Manifests found: {manifests_found}\")\n",
        "\n",
        "if not frames:\n",
        "    raise FileNotFoundError(\n",
        "        \"No manifest.parquet found under selected companies. \"\n",
        "        \"Checked both 'esg_stage' and 'esg_stage1'. \"\n",
        "        \"Verify COMPANIES_ROOT path and folder names like '001. Walmart'.\"\n",
        "    )\n",
        "\n",
        "man = pd.concat(frames, ignore_index=True)\n",
        "if YEAR_LIST:   man = man[man[\"doc_year\"].isin(YEAR_LIST)]\n",
        "if REPORT_TYPE: man = man[man[\"report_type\"].isin(REPORT_TYPE)]\n",
        "man = man.reset_index(drop=True)\n",
        "\n",
        "print(f\"Companies selected: {sorted(set(man['company_code']))}\")\n",
        "print(f\"After selection, docs to process: {len(man)}\")\n",
        "\n",
        "# ---------- Output path ----------\n",
        "def out_path(row):\n",
        "    company_dir = Path(row.get(\"__company_dir__\", \"\"))        # .../FORTUNE 500/001. Walmart\n",
        "    out_dir = company_dir / \"esg_stage2_18(hybridv5)\"\n",
        "    did = (\n",
        "        str(row[\"sha256_16\"])\n",
        "        if pd.notna(row.get(\"sha256_16\"))\n",
        "        else Path(str(row.get(\"text_path\") or row.get(\"pdf_path\") or row.get(\"filename\") or \"doc\")).stem\n",
        "    )\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    return out_dir / f\"esg_stage2_{did}.json\"\n",
        "\n",
        "# ---------- Prepare keywords ----------\n",
        "df_kw = load_concepts(UNIFIED_FILE)\n",
        "if df_kw.empty:\n",
        "    raise FileNotFoundError(f\"No keywords found in {UNIFIED_FILE}. Expect columns: pillar, canonical_concept, keyword.\")\n",
        "kp, keymap, concepts_index = build_keyword_processor(df_kw)\n",
        "\n",
        "# ---------- Main loop ----------\n",
        "proc=skip=0\n",
        "for _,row in man.iterrows():\n",
        "    op = out_path(row)\n",
        "    if ONLY_MISSING and op.exists():\n",
        "        skip += 1\n",
        "        continue\n",
        "\n",
        "    filename = str(row.get(\"filename\") or \"\")\n",
        "    div_code, div_name = get_company_sic_by_name(filename)\n",
        "\n",
        "    sents = read_sentences(row.to_dict())\n",
        "    res   = analyze_doc(\n",
        "        sentences=sents,\n",
        "        kp=kp,\n",
        "        keymap=keymap,\n",
        "        concepts_index=concepts_index,\n",
        "        precomputed_div=(div_code, div_name)\n",
        "    )\n",
        "    res[\"meta\"] = {\n",
        "        \"filename\": row.get(\"filename\"),\n",
        "        \"doc_year\": int(row.get(\"doc_year\")) if pd.notna(row.get(\"doc_year\")) else None,\n",
        "        \"report_type\": row.get(\"report_type\"),\n",
        "        \"company_code\": int(row.get(\"company_code\")) if pd.notna(row.get(\"company_code\")) else None,\n",
        "        \"company_dir\": row.get(\"__company_dir__\", \"\"),\n",
        "        \"stage1_dir\": row.get(\"__base_dir__\", \"\")\n",
        "    }\n",
        "    with open(op,\"w\",encoding=\"utf-8\") as f: json.dump(res,f,ensure_ascii=False,indent=2)\n",
        "    proc+=1\n",
        "    print(f\"Processed {proc}: wrote {op}\")\n",
        "\n",
        "print(f\"✅ Done. Wrote {proc}; skipped {skip}. Results saved in each company/esg_stage2_18(hybridv5)/\")"
      ],
      "metadata": {
        "id": "4CW5cqJ-oeOf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}