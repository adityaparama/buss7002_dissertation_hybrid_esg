{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giF-wG2DLg6I",
        "outputId": "cbfc78e5-046b-44be-de5e-3bcaa74be087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Company directories found under COMPANIES_ROOT: 501\n",
            "Companies scanned (after name check & FOLDER_SELECT): 501\n",
            "Manifests found: 5\n",
            "Companies selected: [6, 7, 8, 9, 10]\n",
            "After selection, docs to process: 28\n",
            "Processed 1: wrote /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/006. ExxonMobil/esg_stage2/esg_stage2_f125df312bf1444f.json\n",
            "Processed 2: wrote /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/006. ExxonMobil/esg_stage2/esg_stage2_8910827c98147d81.json\n",
            "Processed 3: wrote /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/006. ExxonMobil/esg_stage2/esg_stage2_93b3ce40633fe859.json\n",
            "Processed 4: wrote /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/006. ExxonMobil/esg_stage2/esg_stage2_faafd0ad9425bfb4.json\n",
            "Processed 5: wrote /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/006. ExxonMobil/esg_stage2/esg_stage2_8a79decedd1d5c31.json\n",
            "Processed 6: wrote /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/006. ExxonMobil/esg_stage2/esg_stage2_09941068688471b1.json\n",
            "Processed 7: wrote /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/007. Alphabet/esg_stage2/esg_stage2_328cb5eef576c2f9.json\n",
            "Processed 8: wrote /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/007. Alphabet/esg_stage2/esg_stage2_94556c1db1265670.json\n",
            "Processed 9: wrote /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/007. Alphabet/esg_stage2/esg_stage2_ee666a50db325eba.json\n",
            "Processed 10: wrote /content/drive/MyDrive/Australia Award Scholarship/USYD/Dissertation/DATA/FORTUNE 500/007. Alphabet/esg_stage2/esg_stage2_887517153971a58b.json\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# Stage 2 — ESG Keywords Extraction\n",
        "# ===============================\n",
        "!pip -q install openai pandas numpy nltk pyarrow\n",
        "\n",
        "from google.colab import drive; drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os, re, json, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "import nltk; nltk.download(\"punkt\", quiet=True)\n",
        "import nltk; nltk.download(\"punkt_tab\", quiet=True)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from openai import OpenAI\n",
        "\n",
        "# ---------- Config ----------\n",
        "COMPANIES_ROOT = \"<<Google Drive Link>>\"  # I uploaded the ESG report PDF files on Google Drive. Each company has 1 folder\n",
        "UNIFIED_FILE   = \"<<Google Drive Link>>/unified_keywords.csv\"\n",
        "\n",
        "# Subset of company folders --> to use for filtering company folder for analysis\n",
        "FOLDER_SELECT  = \"006-010\"\n",
        "\n",
        "YEAR_LIST   = []                   # e.g., [2022, 2023]; [] = no filter\n",
        "REPORT_TYPE = []                   # e.g., [\"ESG\", \"AR\" (Annual Report),\"10K\" (10K Filings)]; [] = no filter\n",
        "\n",
        "EMB_MODEL    = \"text-embedding-3-small\"\n",
        "VERIFY_MODEL = \"gpt-4o-mini\"\n",
        "THRESH, HIGH_SIM, DELTA_MARGIN = 0.33, 0.50, 0.05\n",
        "VERIFY_CAP, MAX_KW = 2500, 15\n",
        "MIN_SENT_LEN, MAX_SENT_PER_DOC = 25, 5000\n",
        "\n",
        "# Resumable: if True, skip any doc whose Stage-2 JSON already exists\n",
        "ONLY_MISSING = True\n",
        "\n",
        "DIVISIONS = [\n",
        "    (\"A\",\"Agriculture, Forestry & Fishing\"), (\"B\",\"Mining\"), (\"C\",\"Construction\"),\n",
        "    (\"D\",\"Manufacturing\"), (\"E\",\"Transportation & Public Utilities\"),\n",
        "    (\"F\",\"Wholesale Trade\"), (\"G\",\"Retail Trade\"),\n",
        "    (\"H\",\"Finance, Insurance & Real Estate\"), (\"I\",\"Services\"), (\"J\",\"Public Administration\"),\n",
        "]\n",
        "\n",
        "# ---------- OpenAI ----------\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OpenAPI\")\n",
        "client = OpenAI()\n",
        "\n",
        "# ---------- Ontology / profiles ----------\n",
        "def load_concepts(path=UNIFIED_FILE):\n",
        "    df = pd.read_csv(path, usecols=[\"pillar\",\"canonical_concept\",\"keyword\"]).dropna()\n",
        "    concepts = {}\n",
        "    for (p, cc), g in df.groupby([\"pillar\",\"canonical_concept\"]):\n",
        "        kw = sorted(set(map(str, g[\"keyword\"])))[:MAX_KW]\n",
        "        concepts.setdefault(p, {})[cc] = kw\n",
        "    return concepts\n",
        "CONCEPTS = load_concepts()\n",
        "\n",
        "def build_profiles(concepts):\n",
        "    texts, index = [], []\n",
        "    for p, d in concepts.items():\n",
        "        for cc, kws in d.items():\n",
        "            texts.append(f\"PILLAR:{p}. CONCEPT:{cc}. Related terms: {', '.join(kws)}\")\n",
        "            index.append((p, cc))\n",
        "    return texts, index\n",
        "\n",
        "# ---------- Embeddings / utils ----------\n",
        "def embed_batched(texts, model=EMB_MODEL, bsz=512):\n",
        "    if not texts: return np.zeros((0,1536))\n",
        "    out=[]\n",
        "    for i in range(0, len(texts), bsz):\n",
        "        r = client.embeddings.create(model=model, input=texts[i:i+bsz])\n",
        "        out.extend([d.embedding for d in r.data])\n",
        "    return np.array(out)\n",
        "\n",
        "def cosine_matrix(A,B):\n",
        "    A = A/(np.linalg.norm(A,axis=1,keepdims=True)+1e-12)\n",
        "    B = B/(np.linalg.norm(B,axis=1,keepdims=True)+1e-12)\n",
        "    return A @ B.T\n",
        "\n",
        "def _resolve(path_str: str, base_dir: Path) -> str:\n",
        "    if not path_str or not isinstance(path_str, str):\n",
        "        return \"\"\n",
        "    p = Path(path_str)\n",
        "    if p.is_absolute():\n",
        "        return str(p)\n",
        "    return str((base_dir / p).resolve())\n",
        "\n",
        "def read_sentences(row):\n",
        "    base_dir = Path(row.get(\"__base_dir__\", \"\"))  # per-company stage dir (esg_stage or esg_stage1)\n",
        "    s = row.get(\"sentences_path\"); t = row.get(\"text_path\")\n",
        "    sents=[]\n",
        "    if s and isinstance(s,str):\n",
        "        s_abs = _resolve(s, base_dir)\n",
        "        if os.path.exists(s_abs):\n",
        "            df = pd.read_parquet(s_abs, columns=[\"text\"])\n",
        "            sents = [str(x).strip() for x in df[\"text\"].tolist()]\n",
        "    if not sents and t and isinstance(t,str):\n",
        "        t_abs = _resolve(t, base_dir)\n",
        "        if os.path.exists(t_abs):\n",
        "            txt=open(t_abs,\"r\",encoding=\"utf-8\").read(); txt=re.sub(r\"\\s+\",\" \",txt).strip()\n",
        "            sents=[x.strip() for x in sent_tokenize(txt)]\n",
        "    sents=[x for x in sents if len(x)>=MIN_SENT_LEN]\n",
        "    return sents[:MAX_SENT_PER_DOC] if MAX_SENT_PER_DOC else sents\n",
        "\n",
        "# ---------- Agents (SIC division, pillar verifier, referee) ----------\n",
        "DIV_LIST = \"\\n- \".join([f\"{c} — {n}\" for c,n in DIVISIONS])\n",
        "def classify_division(sentences, take=60):\n",
        "    ctx=\" \".join(sentences[:take])[:6000]\n",
        "    prompt=(f\"Choose ONE SIC Division from:\\n- {DIV_LIST}\\n\"\n",
        "            'Return ONLY JSON: {\"code\":\"A-J\",\"name\":\"...\",\"confidence\":0.0-1.0}.\\n\\nTEXT:'+ctx)\n",
        "    r=client.chat.completions.create(model=VERIFY_MODEL,temperature=0,\n",
        "        response_format={\"type\":\"json_object\"},messages=[{\"role\":\"user\",\"content\":prompt}])\n",
        "    try:\n",
        "        o=json.loads(r.choices[0].message.content)\n",
        "        return o.get(\"code\",\"I\"), o.get(\"name\",\"Services\"), float(o.get(\"confidence\",0))\n",
        "    except:\n",
        "        return \"I\",\"Services\",0.0\n",
        "\n",
        "class PillarAgent:  # The agent to classify the pillar might consume a lot of resources\n",
        "    def __init__(self,p): self.p=p\n",
        "    def verify(self,sent,concept):\n",
        "        prompt=(\"Return ONLY JSON {\\\"supports\\\":true|false,\\\"reason\\\":\\\"<=15 words\\\"}.\\n\"\n",
        "                \"Does SENTENCE support CONCEPT for PILLAR?\\n\"\n",
        "                f\"PILLAR:{self.p}\\nCONCEPT:{concept}\\nSENTENCE:{sent}\")\n",
        "        r=client.chat.completions.create(model=VERIFY_MODEL,temperature=0,\n",
        "            response_format={\"type\":\"json_object\"},messages=[{\"role\":\"user\",\"content\":prompt}],max_tokens=60)\n",
        "        try:\n",
        "            o=json.loads(r.choices[0].message.content); return bool(o.get(\"supports\",False)), o.get(\"reason\",\"\")\n",
        "        except:\n",
        "            return False,\"parse_error\"\n",
        "\n",
        "class Referee: # The referee agent to ensure no duplicates between E/S/G sentences might consume a lot of resources\n",
        "    def decide(self,sent,a,b):\n",
        "        prompt=('Return ONLY JSON {\"assign_to\":\"environmental|social|governance|none\",\"reason\":\"<=15 words\"}.\\n'\n",
        "                f\"SENTENCE:{sent}\\nA:{a['pillar']}:{a['concept']} ({a['reason']})\\nB:{b['pillar']}:{b['reason']}\")\n",
        "        r=client.chat.completions.create(model=VERIFY_MODEL,temperature=0,\n",
        "            response_format={\"type\":\"json_object\"},messages=[{\"role\":\"user\",\"content\":prompt}],max_tokens=60)\n",
        "        try:\n",
        "            o=json.loads(r.choices[0].message.content); return o.get(\"assign_to\",\"none\")\n",
        "        except:\n",
        "            return \"none\"\n",
        "\n",
        "# ---------- Quant detection ----------  --> This detection is to avoid page number, year, date, and other singular number as 'quantitative sentence'\n",
        "FOOTNOTE_LEAD = re.compile(r\"^\\s*(?:\\(?\\d+\\)?|\\[\\d+\\]|[¹²³⁴⁵⁶⁷⁸⁹])\\s+\")\n",
        "PAGINATION = re.compile(r\"\\b(page|pp\\.|p\\.|appendix|annex|exhibit|figure|table|chapter|section|see page)\\b\", re.I)\n",
        "TOC_LINE   = re.compile(r\"\\.{3,}\\s*\\d{1,4}$\")\n",
        "DATE_HINT  = re.compile(r\"\\b(19|20)\\d{2}\\b|\\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec|fy|q[1-4])\\b\", re.I)\n",
        "NUM_PAT    = re.compile(r\"[+-]?\\d{1,3}(?:[,\\s]\\d{3})*(?:\\.\\d+)?|[+-]?\\d+(?:\\.\\d+)?\")\n",
        "METRIC_HINT = re.compile(\n",
        "    r\"\"\"(?ix)\n",
        "    (tco2e?|co2e?|ghg|emission|scope\\s*[123]|carbon|methane|ch4|\n",
        "     kwh|mwh|gwh|wh|gj|mj|energy|electricity|renewable|\n",
        "     m3|m2|m²|lit(?:re|er)s?|water|withdrawals?|discharge|\n",
        "     kg|t|tonnes?|metric\\s*tons?|waste|recycled|landfill|\n",
        "     incident[s]?|injur(?:y|ies)|fatalit(?:y|ies)|spills?|leaks?|\n",
        "     fine[s]?|penalt(?:y|ies)|hours|training|turnover|absenteeism|\n",
        "     diversity|board|pay|remuneration|audit|briber|ethic|\n",
        "     usd|aud|eur|cad|sgd|yen|cny|inr|zar|mxn|brl|sek|nok|dkk|\n",
        "     us\\$|a\\$|\\$|£|¥|€)\n",
        "    \"\"\")\n",
        "WINDOW_METRIC = re.compile(r\"(?:^|\\b)(?:rate|intensity|emissions?|energy|water|waste|injur|incident|fine|penalt|spent|invested|allocated|revenue|cost|capex)\\b\", re.I)\n",
        "\n",
        "def clean_for_quant(s:str)->str:\n",
        "    s = FOOTNOTE_LEAD.sub(\"\", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "QF_SHOTS = [\n",
        "  {\"role\":\"user\",\"content\":\"Is this quantitative? Years/quarters/pages alone are NOT quantitative.\\nText: We aim for net zero by 2050.\"},\n",
        "  {\"role\":\"assistant\",\"content\":json.dumps({\"quantitative\":False})},\n",
        "  {\"role\":\"user\",\"content\":\"Text: Scope 1 emissions decreased to 0.9 MtCO2e in 2024.\"},\n",
        "  {\"role\":\"assistant\",\"content\":json.dumps({\"quantitative\":True})},\n",
        "  {\"role\":\"user\",\"content\":\"Text: In FY2023 we enhanced our safety culture.\"},\n",
        "  {\"role\":\"assistant\",\"content\":json.dumps({\"quantitative\":False})},\n",
        "  {\"role\":\"user\",\"content\":\"Text: Renewable electricity reached 60% of total consumption.\"},\n",
        "  {\"role\":\"assistant\",\"content\":json.dumps({\"quantitative\":True})},\n",
        "  {\"role\":\"user\",\"content\":\"Text: See page 12 for 2023 results.\"},\n",
        "  {\"role\":\"assistant\",\"content\":json.dumps({\"quantitative\":False})},\n",
        "]\n",
        "\n",
        "def is_quantitative(sentence:str)->bool:\n",
        "    s = clean_for_quant(sentence)\n",
        "    if TOC_LINE.search(s): return False\n",
        "    if PAGINATION.search(s) and not METRIC_HINT.search(s): return False\n",
        "    has_num    = bool(NUM_PAT.search(s))\n",
        "    has_metric = bool(METRIC_HINT.search(s))\n",
        "    if has_metric and has_num: return True\n",
        "    if DATE_HINT.search(s) and not has_metric: return False\n",
        "    if has_num and not (has_metric or WINDOW_METRIC.search(s)): return False\n",
        "    if has_num or has_metric:\n",
        "        prompt = ('Return ONLY JSON {\"quantitative\":true|false}. '\n",
        "                  'Rule: years/quarters/pages alone are NOT quantitative; require a non-date metric.\\n'\n",
        "                  f\"Text: {s}\")\n",
        "        msgs = [*QF_SHOTS, {\"role\":\"user\",\"content\":prompt}]\n",
        "        try:\n",
        "            r = client.chat.completions.create(model=VERIFY_MODEL, temperature=0,\n",
        "                                               response_format={\"type\":\"json_object\"},\n",
        "                                               messages=msgs, max_tokens=12)\n",
        "            o = json.loads(r.choices[0].message.content)\n",
        "            return bool(o.get(\"quantitative\", False))\n",
        "        except:\n",
        "            return False\n",
        "    return False\n",
        "\n",
        "# ---------- Granularity ----------  --> LLM Agent is used to differentiate whether a quantative sentence belongs to general or specific. I think this part use a lot of tokens & processing time\n",
        "SPECIFIC_PAT = re.compile(\n",
        "    r\"\"\"(?ix)\\b(from|to|versus|vs\\.?|compared|baseline|yoy|year[-\\s]?on[-\\s]?year|\n",
        "       reduc(?:e|ed)|increas(?:e|ed)|improv(?:e|ed)|decreas(?:e|ed)|\n",
        "       drop(?:ped)?|rose|grew|cut|achiev(?:e|ed))\\b\"\"\")\n",
        "\n",
        "GR_SHOTS = [\n",
        "  {\"role\":\"user\",\"content\":\"Quantitative: general or specific?\\nText: Reduce emissions 30% by 2030.\"},\n",
        "  {\"role\":\"assistant\",\"content\":json.dumps({\"granularity\":\"General\"})},\n",
        "  {\"role\":\"user\",\"content\":\"Text: Scope 2 fell from 120 GWh to 95 GWh vs a 2022 baseline.\"},\n",
        "  {\"role\":\"assistant\",\"content\":json.dumps({\"granularity\":\"Specific\"})},\n",
        "  {\"role\":\"user\",\"content\":\"Text: Injury rate improved to less than 5% per million hours.\"},\n",
        "  {\"role\":\"assistant\",\"content\":json.dumps({\"granularity\":\"General\"})},\n",
        "  {\"role\":\"user\",\"content\":\"Text: Water withdrawals decreased by 18% year-on-year from 10.5 to 8.6 million m3.\"},\n",
        "  {\"role\":\"assistant\",\"content\":json.dumps({\"granularity\":\"Specific\"})},\n",
        "]\n",
        "\n",
        "def granularity_q(sentence:str)->str:\n",
        "    s = clean_for_quant(sentence)\n",
        "    if NUM_PAT.search(s) and SPECIFIC_PAT.search(s): return \"Specific\"\n",
        "    prompt = ('Return ONLY JSON {\"granularity\":\"General|Specific\"}. '\n",
        "              \"Specific = explicit change/baseline/comparison; General = single level/target without comparison.\\n\"\n",
        "              f\"Text: {s}\")\n",
        "    msgs = [*GR_SHOTS, {\"role\":\"user\",\"content\":prompt}]\n",
        "    try:\n",
        "        r = client.chat.completions.create(model=VERIFY_MODEL, temperature=0,\n",
        "                                           response_format={\"type\":\"json_object\"},\n",
        "                                           messages=msgs, max_tokens=12)\n",
        "        o = json.loads(r.choices[0].message.content)\n",
        "        g = str(o.get(\"granularity\",\"General\")).lower()\n",
        "        return \"Specific\" if g.startswith(\"spec\") else \"General\"\n",
        "    except:\n",
        "        return \"General\"\n",
        "\n",
        "# ---------- Per-document (label the same SIC Division within one folder) ----------\n",
        "def analyze_doc(sentences, precomputed_div=None):\n",
        "    if not sentences:\n",
        "        return {\"sic_division\":None,\"sentences\":0,\"coverage\":{},\"status_by_pillar\":{}}\n",
        "\n",
        "    #  Use precomputed (company-level) SIC if provided; else classify once here\n",
        "    if precomputed_div:\n",
        "        div_code, div_name = precomputed_div\n",
        "    else:\n",
        "        div_code, div_name, _ = classify_division(sentences)\n",
        "\n",
        "    profiles, idx = build_profiles(CONCEPTS)\n",
        "    S = cosine_matrix(embed_batched(sentences), embed_batched(profiles, bsz=64))\n",
        "\n",
        "    # boost likely-quantitative sentences slightly\n",
        "    num_hint = np.array([bool(NUM_PAT.search(s) or METRIC_HINT.search(s)) for s in sentences])\n",
        "    if num_hint.any(): S[num_hint,:] = np.minimum(S[num_hint,:] * 1.08, 1.0)\n",
        "\n",
        "    cand = np.argwhere(S>=THRESH)\n",
        "    agents = {p:PillarAgent(p) for p in CONCEPTS.keys()}\n",
        "    ref    = Referee()\n",
        "\n",
        "    claims, borderline = [], []\n",
        "    for si,ci in cand:\n",
        "        si,ci=int(si),int(ci); sim=float(S[si,ci]); p,c=idx[ci]; s=sentences[si]\n",
        "        (claims if sim>=HIGH_SIM else borderline).append((si,ci,sim,p,c,s))\n",
        "    if len(borderline)>VERIFY_CAP:\n",
        "        borderline = sorted(borderline, key=lambda x:x[2], reverse=True)[:VERIFY_CAP]\n",
        "\n",
        "    accepted=[]\n",
        "    for si,ci,sim,p,c,s in claims:\n",
        "        accepted.append({\"sid\":si,\"sent\":s,\"pillar\":p,\"concept\":c,\"sim\":sim,\"reason\":\"high_similarity\"})\n",
        "    for si,ci,sim,p,c,s in borderline:\n",
        "        ok,reason = agents[p].verify(s,c)\n",
        "        if ok: accepted.append({\"sid\":si,\"sent\":s,\"pillar\":p,\"concept\":c,\"sim\":sim,\"reason\":reason})\n",
        "\n",
        "    # anti double-count per sentence\n",
        "    assigned={}\n",
        "    tmp={}\n",
        "    for a in accepted: tmp.setdefault(a[\"sid\"],[]).append(a)\n",
        "    for sid,arr in tmp.items():\n",
        "        if len(arr)==1: assigned[sid]=arr[0]\n",
        "        else:\n",
        "            arr=sorted(arr,key=lambda x:x[\"sim\"],reverse=True)\n",
        "            if arr[0][\"sim\"]-arr[1][\"sim\"]>=DELTA_MARGIN: assigned[sid]=arr[0]\n",
        "            else:\n",
        "                who=ref.decide(sentences[sid],arr[0],arr[1])\n",
        "                if who!=\"none\":\n",
        "                    pick=next((x for x in arr if x[\"pillar\"]==who),None)\n",
        "                    if pick: assigned[sid]=pick\n",
        "\n",
        "    # coverage with Quant-wins (Specific > General > Qualitative)\n",
        "    coverage = {p:{cc:{\n",
        "        \"count\":0,\"examples\":[],\n",
        "        \"qual_examples\":[],\"qgen_examples\":[],\"qspec_examples\":[],\n",
        "        \"n_qual\":0,\"n_qgen\":0,\"n_qspec\":0,\n",
        "        \"labels\":set(),\"final_label\":\"None\"\n",
        "    } for cc in CONCEPTS[p]} for p in CONCEPTS}\n",
        "\n",
        "    for _, cl in assigned.items():\n",
        "        p, cc, s = cl[\"pillar\"], cl[\"concept\"], cl[\"sent\"]\n",
        "        st = coverage[p][cc]; st[\"count\"] += 1\n",
        "        if is_quantitative(s):\n",
        "            g = granularity_q(s)\n",
        "            if g==\"Specific\":\n",
        "                st[\"n_qspec\"] += 1;\n",
        "                if len(st[\"qspec_examples\"])<3: st[\"qspec_examples\"].append(s)\n",
        "                st[\"labels\"].add(\"Quantitative (Specific)\")\n",
        "            else:\n",
        "                st[\"n_qgen\"]  += 1;\n",
        "                if len(st[\"qgen_examples\"])<3: st[\"qgen_examples\"].append(s)\n",
        "                st[\"labels\"].add(\"Quantitative (General)\")\n",
        "        else:\n",
        "            st[\"n_qual\"] += 1\n",
        "            if len(st[\"qual_examples\"])<3: st[\"qual_examples\"].append(s)\n",
        "            st[\"labels\"].add(\"Qualitative\")\n",
        "\n",
        "    for p,d in coverage.items():\n",
        "        for cc,st in d.items():\n",
        "            if   \"Quantitative (Specific)\" in st[\"labels\"]: st[\"final_label\"]=\"Quantitative (Specific)\"\n",
        "            elif \"Quantitative (General)\" in st[\"labels\"]:  st[\"final_label\"]=\"Quantitative (General)\"\n",
        "            elif \"Qualitative\" in st[\"labels\"]:             st[\"final_label\"]=\"Qualitative\"\n",
        "            else:                                           st[\"final_label\"]=\"None\"\n",
        "            st[\"labels\"] = list(st[\"labels\"])\n",
        "\n",
        "    status = {p:{\n",
        "        \"satisfied\":[cc for cc,st in d.items() if st[\"count\"]>0],\n",
        "        \"missing\"  :[cc for cc,st in d.items() if st[\"count\"]==0]\n",
        "    } for p,d in coverage.items()}\n",
        "\n",
        "    return {\"sic_division\":{\"code\":div_code,\"name\":div_name},\n",
        "            \"sentences\":len(sentences),\n",
        "            \"coverage\":coverage,\n",
        "            \"status_by_pillar\":status}\n",
        "\n",
        "# ---------- Folder-selection ----------\n",
        "def parse_folder_select(selector: str):\n",
        "    if not selector or not str(selector).strip():\n",
        "        return None\n",
        "    keep = set()\n",
        "    for tok in re.split(r\"[,\\s]+\", selector.strip()):\n",
        "        if not tok: continue\n",
        "        m = re.fullmatch(r\"(\\d{3})-(\\d{3})\", tok)\n",
        "        if m:\n",
        "            a, b = int(m.group(1)), int(m.group(2))\n",
        "            if a <= b: keep.update(range(a, b + 1))\n",
        "            else:      keep.update(range(b, a + 1))\n",
        "            continue\n",
        "        m2 = re.fullmatch(r\"(\\d{3})\", tok)\n",
        "        if m2:\n",
        "            keep.add(int(m2.group(1)))\n",
        "    keep = {x for x in keep if 1 <= x <= 500}\n",
        "    return keep or None\n",
        "\n",
        "def extract_code_from_company_dirname(name: str):\n",
        "    m = re.match(r\"^\\s*(\\d{3})\\b\", name or \"\")\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "# ---------- Build manifest across companies for later analysis ----------\n",
        "companies_root = Path(COMPANIES_ROOT)\n",
        "allowed_codes = parse_folder_select(FOLDER_SELECT)\n",
        "\n",
        "company_dirs = [d for d in companies_root.iterdir() if d.is_dir()]\n",
        "print(f\"Company directories found under COMPANIES_ROOT: {len(company_dirs)}\")\n",
        "\n",
        "frames = []\n",
        "manifests_found = 0\n",
        "checked = 0\n",
        "\n",
        "for cdir in sorted(company_dirs):\n",
        "    checked += 1\n",
        "    code = extract_code_from_company_dirname(cdir.name)\n",
        "    if code is None:\n",
        "        continue\n",
        "    if allowed_codes is not None and code not in allowed_codes:\n",
        "        continue\n",
        "\n",
        "    stage_dir = None\n",
        "    mpath = None\n",
        "    for candidate in (\"esg_stage\", \"esg_stage1\"):\n",
        "        cand_dir = cdir / candidate\n",
        "        cand_manifest = cand_dir / \"manifest.parquet\"\n",
        "        if cand_manifest.exists():\n",
        "            stage_dir = cand_dir\n",
        "            mpath = cand_manifest\n",
        "            break\n",
        "    if stage_dir is None:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        df = pd.read_parquet(mpath)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Could not read manifest for {cdir.name}: {e}\")\n",
        "        continue\n",
        "\n",
        "    df = df[df[\"status\"].isin([\"ok_native\",\"ok_ocr\",\"ok_hybrid\",\"ok\"])].copy()\n",
        "    if df.empty:\n",
        "        continue\n",
        "\n",
        "    df[\"company_code\"] = code\n",
        "    df[\"__base_dir__\"] = str(stage_dir.resolve())  # points to company/esg_stage* (for _resolve)\n",
        "    df[\"__company_dir__\"] = str(cdir.resolve())    # points to company/ (for output dir esg_stage2)\n",
        "    frames.append(df)\n",
        "    manifests_found += 1\n",
        "\n",
        "print(f\"Companies scanned (after name check & FOLDER_SELECT): {checked}\")\n",
        "print(f\"Manifests found: {manifests_found}\")\n",
        "\n",
        "if not frames:\n",
        "    raise FileNotFoundError(\n",
        "        \"No manifest.parquet found under selected companies. \"\n",
        "        \"Checked both 'esg_stage' and 'esg_stage1'. \"\n",
        "        \"Verify COMPANIES_ROOT path and folder names like '001. Walmart'.\"\n",
        "    )\n",
        "\n",
        "man = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "if YEAR_LIST:   man = man[man[\"doc_year\"].isin(YEAR_LIST)]\n",
        "if REPORT_TYPE: man = man[man[\"report_type\"].isin(REPORT_TYPE)]\n",
        "man = man.reset_index(drop=True)\n",
        "\n",
        "print(f\"Companies selected: {sorted(set(man['company_code']))}\")\n",
        "print(f\"After selection, docs to process: {len(man)}\")\n",
        "\n",
        "# ---------- Output path: save JSON in company/esg_stage2/ ----------\n",
        "def out_path(row):\n",
        "    company_dir = Path(row.get(\"__company_dir__\", \"\"))        # .../FORTUNE 500/001. Walmart\n",
        "    out_dir = company_dir / \"esg_stage2\"                      # sibling of esg_stage / esg_stage1\n",
        "    did = (\n",
        "        str(row[\"sha256_16\"])\n",
        "        if pd.notna(row.get(\"sha256_16\"))\n",
        "        else Path(str(row.get(\"text_path\") or row.get(\"pdf_path\") or row.get(\"filename\") or \"doc\")).stem\n",
        "    )\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    return out_dir / f\"esg_stage2_{did}.json\"\n",
        "\n",
        "# ---------- SIC cache: classify once per company, reuse ----------\n",
        "SIC_CACHE = {}  # { company_code: (div_code, div_name) }\n",
        "\n",
        "def get_company_sic(row):\n",
        "    code = int(row.get(\"company_code\")) if pd.notna(row.get(\"company_code\")) else None\n",
        "    if code in SIC_CACHE:\n",
        "        return SIC_CACHE[code]\n",
        "    # pick this row as the sample doc for the company\n",
        "    sents = read_sentences(row.to_dict())\n",
        "    if not sents:\n",
        "        # fallback: assume Services if empty\n",
        "        SIC_CACHE[code] = (\"I\", \"Services\")\n",
        "        return SIC_CACHE[code]\n",
        "    div_code, div_name, _ = classify_division(sents)\n",
        "    SIC_CACHE[code] = (div_code, div_name)\n",
        "    return SIC_CACHE[code]\n",
        "\n",
        "# ---------- Main loop ----------\n",
        "proc=skip=0\n",
        "for _,row in man.iterrows():\n",
        "    op = out_path(row)\n",
        "    if ONLY_MISSING and op.exists():\n",
        "        skip += 1\n",
        "        continue\n",
        "\n",
        "    # company-level SIC (computed once per company)\n",
        "    pre_sic = get_company_sic(row)  # tuple (div_code, div_name)\n",
        "\n",
        "    sents = read_sentences(row.to_dict())\n",
        "    res   = analyze_doc(sents, precomputed_div=pre_sic)\n",
        "    res[\"meta\"] = {\n",
        "        \"filename\": row.get(\"filename\"),\n",
        "        \"doc_year\": int(row.get(\"doc_year\")) if pd.notna(row.get(\"doc_year\")) else None,\n",
        "        \"report_type\": row.get(\"report_type\"),\n",
        "        \"company_code\": int(row.get(\"company_code\")) if pd.notna(row.get(\"company_code\")) else None,\n",
        "        \"company_dir\": row.get(\"__company_dir__\", \"\"),\n",
        "        \"stage1_dir\": row.get(\"__base_dir__\", \"\")\n",
        "    }\n",
        "    with open(op,\"w\",encoding=\"utf-8\") as f: json.dump(res,f,ensure_ascii=False,indent=2)\n",
        "    proc+=1\n",
        "    print(f\"Processed {proc}: wrote {op}\")\n",
        "\n",
        "print(f\"Done. Wrote {proc}; skipped {skip}. Results saved in each company/esg_stage2/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZBGWPZ72lavp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}